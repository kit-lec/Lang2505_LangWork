{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d56785dc-8934-428d-ae51-82aba42a2838",
   "metadata": {},
   "source": [
    "# LLM and ChatModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d948f23d-1476-4e94-b197-725a16e5d72a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# LangChain  관련 주요 링크\n",
    "\n",
    "-  Python Langchain 공식 홈:  https://python.langchain.com/\n",
    "-  API 레퍼런스 홈: https://python.langchain.com/api_reference/reference.html\n",
    "\n",
    "\n",
    "## Langchain 의 패키지 구성\n",
    "\n",
    "\n",
    "### Base Packages\n",
    "- [Core: langchain-core](https://python.langchain.com/api_reference/core)\n",
    "- [Langchain: langchain](https://python.langchain.com/api_reference/langchain)\n",
    "- [Test Splitters: langchain-text-splitters](https://python.langchain.com/api_reference/text_splitters)\n",
    "- [Community: langchain-community](https://python.langchain.com/api_reference/community)\n",
    "- [Experimental: langchain-experimental](https://python.langchain.com/api_reference/experimental)\n",
    "\n",
    "### Integrations\n",
    "- 랭체인은 수많은 LLM 모델들과 커뮤니티, 벡터스토어, 데이터베이스, 툴 들과 함께 사용할수 있도록 제공되는 패키지들이 많다 (앞으로 더 많아 질거다)\n",
    "- [OpanAI: langchain-openai](https://python.langchain.com/api_reference/openai)\n",
    "- [Huggingface: langchain-huggingface](https://python.langchain.com/api_reference/huggingface)\n",
    "- [MistalAI: langchain-mistralai](https://python.langchain.com/api_reference/mistralai)\n",
    "- 그밖에도 많이 있다 ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc357e1b-6e09-455b-9328-e3574203d48e",
   "metadata": {},
   "source": [
    "# 환경변수 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b55641f-c5df-4bd5-a966-2f984e682f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa33b18d-096b-4ed1-b65a-eecadd8bbe7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sk-proj-iKU13YeoxNgF...\n"
     ]
    }
   ],
   "source": [
    "print(f'{os.environ['OPENAI_API_KEY'][:20]}...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e7469d-dbc9-4a56-bc79-49143407370b",
   "metadata": {},
   "source": [
    "# ■ LLM vs. Chat model\n",
    "\n",
    "LangChain 은 LLM 과 Chat model 두가지를 지원합니다\n",
    "\n",
    "`LLM`(Large Language Model)과 `Chat Model`은 비슷한 역할을 하지만, 약간의 차이점이 있습니다.\n",
    "\n",
    "이는 주로 **모델의 입력 및 상호작용 방식**에서 나타난다.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. LLM (Large Language Model)\n",
    "- **특징**:\n",
    "  - 일반적으로 **텍스트 입력**을 받고, 이에 대한 텍스트 출력을 생성합니다.\n",
    "  - 단순한 프롬프트 기반 입력/출력을 처리하기 위한 모델입니다.\n",
    "  - 사용자가 제공한 입력 텍스트를 분석하고, 그에 대한 결과를 한 번에 출력합니다.\n",
    "- **입력 형식**:\n",
    "  ```plaintext\n",
    "  \"Tell me a summary of the benefits of LangChain.\"\n",
    "  ```\n",
    "- **출력 형식**:\n",
    "  ```plaintext\n",
    "  \"LangChain is a framework designed to simplify the development of applications powered by large language models, making it easier to manage prompts, chains, and integrations.\"\n",
    "  ```\n",
    "- **주요 사용 사례**:\n",
    "  - 단일 질문-답변\n",
    "  - 텍스트 생성\n",
    "  - 간단한 프롬프트 처리를 위한 작업\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Chat Model\n",
    "- **특징**:\n",
    "  - **대화 형식**으로 설계된 모델로, 다중 턴 대화를 처리할 수 있다.\n",
    "  - 입력 형식이 **메시지 Message**로 구성되며, 각 메시지는 사용자의 메시지 (User Message)와 시스템의 메시지(System Message)로 나뉜다.\n",
    "  - '문맥'을 이해하고 '대화의 흐름'을 유지하는 데 최적화되어 있다.\n",
    "- **입력 형식**:\n",
    "  메시지 객체를 전달해야 하며, 보통 아래와 같은 구조입니다.\n",
    "  ```python\n",
    "  [\n",
    "      {\"role\": \"system\", \"content\": \"You are an assistant who helps with Python programming.\"},\n",
    "      {\"role\": \"user\", \"content\": \"Can you explain the difference between LLM and chat models in LangChain?\"}\n",
    "  ]\n",
    "  ```\n",
    "- **출력 형식**:\n",
    "  ```python\n",
    "  {\"role\": \"assistant\", \"content\": \"Sure! LLM and Chat Models differ in their input and interaction styles...\"}\n",
    "  ```\n",
    "- **주요 사용 사례**:\n",
    "  - 다중 턴 대화\n",
    "  - 문맥 추적 및 유지 (대화 히스토리 반영)\n",
    "  - 대화 기반 챗봇, FAQ 시스템 등\n",
    "\n",
    "---\n",
    "\n",
    "### 3. 주요 차이점 요약\n",
    "| **특징**        | **LLM**                                                | **Chat Model**                                        |\n",
    "|-----------------|------------------------------------------------------|----------------------------------------------------|\n",
    "| **입력 형식**   | 단일 텍스트 입력                                         | 역할 기반의 대화 메시지 객체 (role: system, user, assistant) |\n",
    "| **대화 히스토리**| 문맥 추적 불가능 (단일 요청 처리)                          | 대화 히스토리를 통해 문맥을 유지하고 반영                 |\n",
    "| **사용 목적**   | 텍스트 생성, 요약, 단순 질의응답                             | 대화형 인터페이스, 챗봇, 다중 턴 질의응답               |\n",
    "| **응용 사례**   | 단일 질문-답변, 텍스트 생성                                | 고객 지원 챗봇, 인터랙티브 Q&A, 멀티턴 대화 시스템          |\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### 5. 언제 어떤 것을 선택해야 할까요?\n",
    "- **단일 작업이나 간단한 텍스트 생성**:\n",
    "  - `LLM`을 사용하는 것이 적합합니다.\n",
    "- **대화 기반 애플리케이션이나 문맥을 유지해야 하는 작업**:\n",
    "  - `Chat Model`을 사용하는 것이 더 적합합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11badd6-ebc2-4a45-b1d4-34e9fa44090e",
   "metadata": {},
   "source": [
    "# LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a772a3b-6e39-471d-bc50-28fa86508ad1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.3.23'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import langchain\n",
    "langchain.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9545aef-1c32-476e-ba41-f7559d5f61d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM 모델\n",
    "from langchain_openai.llms.base import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e6099f2-0777-46c2-9401-1816c849eb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatModel\n",
    "from langchain_openai.chat_models.base import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21267cbd-3f8e-4e39-90d4-f9aa1c37ffcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI()   # OPENAI_API_KEY 필요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6bece8b-3f39-498e-9636-85f53d6f50f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gpt-3.5-turbo-instruct'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7e6bd7e-502e-4cee-a744-399b5aee7574",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f481cc31-e087-4f1a-897f-f165fc29f0f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gpt-3.5-turbo'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef419717-f2b1-45f1-9ea9-b1ff3ea669ae",
   "metadata": {},
   "source": [
    "# LLM 호출 (invoke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "736ad35f-d6ad-4deb-b8e3-bd0355d3b446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "답변 \n",
      "\n",
      "There are currently eight planets in our solar system: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and Neptune. However, there may be more planets beyond our solar system that have not yet been discovered. \n"
     ]
    }
   ],
   "source": [
    "result = llm.invoke(\"How many planets are there?\")  # 입력 str\n",
    "print(type(result))  # 출력 str\n",
    "print('답변', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eccd5eb9-6df0-4e38-b0a1-e7ec69b2aa61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "답변 \n",
      "태양계에는 8개의 행성이 있습니다. 이들은 수성, 금성, 지구, 화성, 목성, 토성, 천왕성, 해왕성입니다. 하지만 최근에는 명왕성을 비롯한 여러 개의 소행성과 미세 행성들도 발견되었습니다.\n"
     ]
    }
   ],
   "source": [
    "result = llm.invoke(\"태양계에는 얼마나 많은 행성들이 있죠?\")  # 입력 str\n",
    "print(type(result))  # 출력 str\n",
    "print('답변', result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5e9794-de7f-493b-b44f-f4f04c801242",
   "metadata": {},
   "source": [
    "# ChatModel 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5df10da-615a-4575-902f-3b7bff22401a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "답변 content='In our solar system, there are 8 planets: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and Neptune.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 29, 'prompt_tokens': 13, 'total_tokens': 42, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BopJL7k9QhssZYWnoHL9xwKTP6L0e', 'finish_reason': 'stop', 'logprobs': None} id='run--78a43e8a-5c6b-432d-b4b7-c4a337347e7f-0' usage_metadata={'input_tokens': 13, 'output_tokens': 29, 'total_tokens': 42, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "💙 In our solar system, there are 8 planets: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and Neptune.\n"
     ]
    }
   ],
   "source": [
    "result = chat.invoke(\"How many planets are there?\")  # 입력 str\n",
    "print(type(result))  # 출력 Message\n",
    "print('답변', result)\n",
    "print('💙', result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb365338-1961-4781-a8ae-dadc9464eaf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "답변 content='태양계에는 총 8개의 행성이 있습니다. 수성, 금성, 지구, 화성, 목성, 토성, 천왕성, 명왕성입니다.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 57, 'prompt_tokens': 32, 'total_tokens': 89, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BopVwvkGbPyWN0w4uqcZGdPZKfVmZ', 'finish_reason': 'stop', 'logprobs': None} id='run--4dbac12d-a982-480c-b4d1-075e7d1c78d2-0' usage_metadata={'input_tokens': 32, 'output_tokens': 57, 'total_tokens': 89, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "💙 태양계에는 총 8개의 행성이 있습니다. 수성, 금성, 지구, 화성, 목성, 토성, 천왕성, 명왕성입니다.\n"
     ]
    }
   ],
   "source": [
    "result = chat.invoke(\"태양계에는 얼마나 많은 행성들이 있죠?\")  # 입력 str\n",
    "print(type(result))  # 출력 Message\n",
    "print('답변', result)\n",
    "print('💙', result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0321dc54-802e-4978-9442-877617858144",
   "metadata": {},
   "source": [
    "## 한글 or 영어 ?\n",
    "\n",
    "챗 GPT의 언어 처리 능력은 인공지능 기술의 훌륭한 발전을 보여줍니다. 하지만 사용자가 받는 답변의 품질은 제출하는 언어에 따라 약간의 차이가 있을 수 있습니다. 이런 차이는 챗 GPT가 학습하는 과정에서 다양한 언어의 데이터 양과 품질, 그리고 언어별 특성을 얼마나 잘 처리하는지에 따라 결정됩니다.\n",
    "\n",
    "OpenAI의 언어 모델, 특히 GPT 시리즈는 다양한 데이터 소스에서 얻은 대량의 데이터로 학습됩니다. 이 데이터는 주로 영어를 비롯한 여러 언어에서 수집되며, 학습 데이터의 구성은 모델의 성능과 일반화 능력에 큰 영향을 미칩니다.\n",
    "\n",
    "영어는 전세계적으로 많이 사용되며*, 인터넷 상의 데이터도 영어가 많아서 챗 GPT는 영어 질문에 대해 더 정확하고 자연스러운 답변을 제공할 확률이 높습니다. 그러나 한국어와 같은 다른 언어는 상대적으로 데이터가 부족하거나, 언어의 복잡성 때문에 처리가 더 어려울 수 있어, 이로 인해 답변의 품질에 차이가 나타날 수 있습니다.\n",
    "\n",
    "참고\n",
    "- https://fastcampus.co.kr/gov_review_insightGPTlang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1853dab-e915-44a7-8cfe-bbd44f4a8fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 아래와 같이 api 키를 직접 매개변수로 건네줄수도 있지만...  KEY 비추한다.  환경변수 사용을 추천한다.\n",
    "#\n",
    "# llm = OpenAI(openai_api_key=\"sk-\")\n",
    "# chat = ChatOpenAI(openai_api_key=\"sk-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "201fe7e7-e08a-49df-b90b-d51d6ce68b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic.chat_models import ChatAnthropic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3adea02e-2971-46e8-b690-f87fc19c676d",
   "metadata": {},
   "source": [
    "# Invoke Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "90adc0eb-e797-477e-a832-51f4c38fa691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatModel 은 '질문'만 받는게 아니라 '대화' 도 할수 있다 (Message 를 보낼수도 있다)\n",
    "# '대화(conversation)' 은\n",
    "#    : 여러 메세지 묶음\n",
    "#    : 상대의 대화의 맥락에 맞게 대답할수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccef9399-4db3-4d1b-93bb-e1b6e61f3f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(temperature=0.1)\n",
    "        # 모델의 응답 다양성을 제어하는 역할을 합니다.\n",
    "        # 이는 OpenAI의 GPT 모델에서 사용하는 매개변수로,\n",
    "        #  생성되는 텍스트의 창의성과 확률적 다양성(랜덤성을 조정합니다)ㄴ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbfc997-8e52-4304-a1b0-7e9df6be017e",
   "metadata": {},
   "source": [
    "## Human / System / AI Message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b27dc6e-8e1a-4f3e-8e40-71abdc9981d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages.human import HumanMessage\n",
    "from langchain_core.messages.system import SystemMessage\n",
    "from langchain_core.messages.ai import AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ee687649-db5e-41d9-8998-78d9b4bfbc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HumanMessage : 사람이 AI 에 보내는 Message\n",
    "# SystemMessage : LLM 에 설정들을 제공하기 위한 Message\n",
    "# AIMessage: AI 에 의해 리턴되는 Message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c16df092-c434-4f1c-8068-6d943a46a29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(\n",
    "        content = \"You are a geography expert. And your only reply in Korean\",\n",
    "    ),\n",
    "    AIMessage(\n",
    "        content = \"안녕, 내 이름은 둘리 야\",\n",
    "    ),\n",
    "    HumanMessage(\n",
    "        content = \"\"\"What is the distance between Mexico and Thailand.\n",
    "          Also, what is your name?\"\"\",        \n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c47de9bc-bc1e-4c5c-8ba0-fcac91bd6d93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='멕시코와 태국 사이의 거리는 대략 15,000km입니다. 제 이름은 둘리입니다.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 36, 'prompt_tokens': 58, 'total_tokens': 94, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BopkXrwlCvcJyKrj2SqZ57MfNtd2z', 'finish_reason': 'stop', 'logprobs': None}, id='run--16a1aad9-8787-4b5d-b6b8-6534acb93231-0', usage_metadata={'input_tokens': 58, 'output_tokens': 36, 'total_tokens': 94, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = chat.invoke(messages)  # <- 입력 Messages\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "32a552a9-c492-4bf5-b5b5-e6caffe254d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'멕시코와 태국 사이의 거리는 대략 15,000km입니다. 제 이름은 둘리입니다.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ef9cbe-628a-4a38-8882-4969d3bdef01",
   "metadata": {},
   "source": [
    "# Prompt Template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7021b1e8-04dc-485f-9429-d497f4f17e63",
   "metadata": {},
   "source": [
    "## Prompt\n",
    "↑ messages 를 prompt 라고도 함 (?)\n",
    "- 모델에 입력으로 제공되는 텍스트나 데이터\n",
    "- 모델에게 작업을 수행하도록 지시하거나, 모델이 생성할 텍스트의 컨텍스트를 제공\n",
    "- LLM 과 의사소통하기 위한 방법\n",
    "\n",
    "---\n",
    "\n",
    "LLM(대형 언어 모델)에서 **프롬프트 prompt**란 모델에 **입력**으로 제공되는 텍스트나 데이터입니다. 이는 모델에게 작업을 수행하도록 지시하거나, 모델이 생성할 텍스트의 컨텍스트를 제공합니다. 프롬프트는 모델의 출력을 결정하는 중요한 역할을 합니다.\n",
    "\n",
    "### 프롬프트의 역할:\n",
    "1. **모델에 대한 지시**: 프롬프트는 모델에게 무엇을 해야 할지 알려주는 역할을 합니다. 예를 들어, 사용자가 모델에게 질문을 하거나, 특정 스타일의 텍스트를 생성하도록 요청할 때 프롬프트가 필요합니다.\n",
    "\n",
    "2. **컨텍스트 제공**: 모델이 적절한 응답을 생성할 수 있도록 필요한 배경 정보나 문맥을 제공합니다. 예를 들어, 어떤 주제에 대한 질문을 할 때, 관련 배경 정보를 제공하여 모델이 더 정확한 답을 할 수 있게 합니다.\n",
    "\n",
    "3. **모델의 출력 유도**: 프롬프트가 모델의 출력을 유도하고, 생성되는 텍스트의 스타일, 내용, 형식 등을 결정하는 데 중요한 영향을 미칩니다.\n",
    "\n",
    "### 예시:\n",
    "1. **질문 응답**:\n",
    "   - **프롬프트**: \"What is the capital of France?\"\n",
    "   - **출력**: \"The capital of France is Paris.\"\n",
    "\n",
    "2. **창의적 글쓰기**:\n",
    "   - **프롬프트**: \"Write a short story about a dragon and a knight.\"\n",
    "   - **출력**: 모델이 창의적으로 드래곤과 기사에 관한 이야기를 생성합니다.\n",
    "\n",
    "3. **번역**:\n",
    "   - **프롬프트**: \"Translate the following sentence to Spanish: 'Hello, how are you?'\"\n",
    "   - **출력**: \"Hola, ¿cómo estás?\"\n",
    "\n",
    "### 프롬프트의 종류:\n",
    "- **단순한 질문**: 사용자가 단순히 궁금한 점을 묻는 형태.\n",
    "- **지시문**: 특정 작업을 수행하도록 지시하는 형태.\n",
    "- **형식화된 입력**: 특정 형식이나 구조를 갖춘 입력(예: 텍스트 요약, 번역, 코드 작성 등).\n",
    "\n",
    "### 프롬프트 설계의 중요성:\n",
    "- **정확한 결과**를 얻기 위해서는 **프롬프트의 설계**가 매우 중요합니다. 프롬프트가 모호하거나 불완전하면 모델이 원하는 출력을 생성하기 어렵습니다.\n",
    "- 다양한 프롬프트를 실험하면서 모델의 반응을 관찰하고, 가장 적합한 프롬프트를 찾는 것이 중요합니다.\n",
    "\n",
    "### 프롬프트 설계 팁:\n",
    "1. **명확하고 구체적인 지시**: 무엇을 원하는지 정확하게 전달하세요. 예를 들어, \"Explain quantum mechanics\"보다는 \"Explain quantum mechanics in simple terms for a high school student\"와 같이 구체적인 요구를 하는 것이 좋습니다.\n",
    "   \n",
    "2. **적절한 컨텍스트 제공**: 필요한 배경 정보나 문맥을 제공하면 모델이 더 정확한 답변을 생성할 수 있습니다.\n",
    "\n",
    "3. **다양한 실험**: 프롬프트를 조금씩 바꿔가며 테스트해 보면서 최적의 응답을 유도할 수 있습니다.\n",
    "\n",
    "### 결론:\n",
    "프롬프트는 대형 언어 모델에게 작업을 지시하는 중요한 입력으로, 모델이 수행할 작업의 방향을 결정짓는 요소입니다. 프롬프트를 잘 설계하는 것이 LLM을 효과적으로 활용하는 데 큰 도움이 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86341981-6449-4fb0-a868-82a7873d76bb",
   "metadata": {},
   "source": [
    "Prompt 성능이 좋다면 LLM 답변의 성능도 좋을 것입니다.\n",
    "\n",
    "모든 웹 사이트들은 상황에 맞는 뛰어는 성능의 prompt 를 제작하는데 전념함.\n",
    "\n",
    "LangChain 은 prompt 를 공유하기 위한 커뮤니티도 형성되고 있다.\n",
    "산업 전체 전반적으로 각 분야별 prompt 를 만들어 내고 있다.\n",
    "\n",
    "예를 들면\n",
    "| 플랫폼               | 기능              | URL                                                             |\n",
    "| ----------------- | --------------- | --------------------------------------------------------------- |\n",
    "| **LangChain Hub** | 프롬프트 및 체인 공유    | [smith.langchain.com/hub](https://smith.langchain.com/hub)      |\n",
    "| **Discord**       | 커뮤니티, 프롬프트 논의   | [discord.gg/langchain](https://discord.gg/langchain)            |\n",
    "| **GitHub**        | 코드 예제, 프롬프트 활용법 | [LangChain Examples](https://github.com/langchain-ai/langchain) |\n",
    "\n",
    "\n",
    "그래서, LangChain 프레임워크의 많은 부분이 prompt 에 집중되어 있다.\n",
    "\n",
    "prompt 끼리 결함도 할수 있고, 저장하거나 불러올수도 있다.\n",
    "\n",
    "변수 설정 도중에 검증도 할수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "327d593e-2aea-4277-85f6-d9b7b0d9cf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# v0.3\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "# https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.prompt.PromptTemplate.html\n",
    "\n",
    "from langchain_core.prompts.chat import ChatMessagePromptTemplate, ChatPromptTemplate\n",
    "# https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html\n",
    "# https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatMessagePromptTemplate.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a215a73e-8db9-4e43-a5ce-3e85e28ef1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatPromptTemplate 는 message(s) 로부터 template 을 만듬.\n",
    "# PromptTemplate 는 string 을 이용해서 template 을 만듬.\n",
    "#  ↑ 둘다 유용하게 쓰임."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9237696a-1891-4af9-83b0-4f578ebc9c67",
   "metadata": {},
   "source": [
    "## PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2832a10d-2cee-43ae-9701-cb184036f7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.prompts.prompt.PromptTemplate'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['country_a', 'country_b'], input_types={}, partial_variables={}, template='What is the distance between {country_a} and {country_b}')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = PromptTemplate.from_template(\n",
    "    # placeholder {...} 사용\n",
    "    \"What is the distance between {country_a} and {country_b}\"    \n",
    ")\n",
    "\n",
    "print(type(template))\n",
    "template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "27094332-6fce-43d9-aa1e-de8ff18dfbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# template.format()  # KeyError: 'country_a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "16da2c4b-0431-4fe8-9416-7884dc045706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the distance between Mexico and Thailand'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = template.format(country_a = 'Mexico', country_b = 'Thailand')\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cc25b337-84af-4cab-b570-5b10f86fdcd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The distance between Mexico and Thailand is approximately 15,332 kilometers (9,528 miles) when measured in a straight line.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 15, 'total_tokens': 41, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-Bopua7A9J6A0Q7ppCQGYOAzJJsHBR', 'finish_reason': 'stop', 'logprobs': None}, id='run--1606ccb6-1082-43fc-9dd4-31272cd61164-0', usage_metadata={'input_tokens': 15, 'output_tokens': 26, 'total_tokens': 41, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2df4e11-2a6d-448c-bf0e-93e3fff10cfd",
   "metadata": {},
   "source": [
    "## ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c64edaa7-74fb-45e1-9847-cb2ee8c20a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.prompts.chat.ChatPromptTemplate'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['country_a', 'country_b', 'language', 'name'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['language'], input_types={}, partial_variables={}, template='You are a geography expert. And your only reply in {language}'), additional_kwargs={}), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=['name'], input_types={}, partial_variables={}, template='안녕, 내 이름은 {name} 야'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['country_a', 'country_b'], input_types={}, partial_variables={}, template='\\n        What is the distance between {country_a} and {country_b}.\\n        Also, what is your name?\\n    '), additional_kwargs={})])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = ChatPromptTemplate.from_messages([\n",
    "    # SystemMessage 튜플\n",
    "    (\"system\", \"You are a geography expert. And your only reply in {language}\"),\n",
    "    \n",
    "    # AIMessage 튜플\n",
    "    (\"ai\", \"안녕, 내 이름은 {name} 야\"),\n",
    "\n",
    "    # HumanMessage 튜플\n",
    "    (\"human\", \"\"\"\n",
    "        What is the distance between {country_a} and {country_b}.\n",
    "        Also, what is your name?\n",
    "    \"\"\"),\n",
    "])\n",
    "\n",
    "print(type(template))\n",
    "template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "621d9361-2d69-48c0-8abe-74a83f01fbeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a geography expert. And your only reply in Korean', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='안녕, 내 이름은 뽀로로 야', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='\\n        What is the distance between Canada and Japan.\\n        Also, what is your name?\\n    ', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = template.format_messages(\n",
    "    language=\"Korean\",\n",
    "    name=\"뽀로로\",\n",
    "    country_a=\"Canada\",\n",
    "    country_b=\"Japan\",\n",
    ")\n",
    "\n",
    "print(type(prompt))\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1e7e8581-680d-4209-adc1-9bbb02bed49c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='캐나다와 일본 사이의 거리는 대략 8058 킬로미터 입니다. 제 이름은 뽀로로입니다.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 40, 'prompt_tokens': 62, 'total_tokens': 102, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-Boq2Y5L83cChJ93ZzkoGLHVnC0BVV', 'finish_reason': 'stop', 'logprobs': None}, id='run--08edd5c3-f64f-4c29-9ef9-67205cc6c060-0', usage_metadata={'input_tokens': 62, 'output_tokens': 40, 'total_tokens': 102, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8046fd04-b89c-47c0-bb1a-e828f4194a6e",
   "metadata": {},
   "source": [
    "# OutParser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b27701-7e6e-48b8-8cc7-81c0d61d82aa",
   "metadata": {},
   "source": [
    "## Output Parser 란\n",
    "\n",
    "LLM(대형 언어 모델)에서 생성된 출력을 처리하고 '원하는 형식으로 변환'하는 데 사용되는 유틸리티입니다. 이를 통해 모델이 생성하는 텍스트를 '구조화된 데이터로 변환'하거나, '특정 규칙에 따라 데이터를 추출'할 수 있습니다\n",
    "\n",
    "1. 출력 구조화\n",
    "    - 모델의 텍스트 응답을 파싱하여 JSON, 딕셔너리, 목록 등과 같은 프로그래밍에서 사용 가능한 구조화된 데이터로 변환합니다\n",
    "    \n",
    "1. 출력 검증\n",
    "    - 모델이 예상치 못한 출력을 반환할 경우 적절한 에러 메시지를 제공하거나 기본값을 반환하도록 처리할 수 있습니다.\n",
    "    \n",
    "1. 출력 표준화\n",
    "    - 언어 모델의 출력이 항상 일관된 형식으로 제공되도록 보장합니다.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a68400-da32-472c-b8c7-c8b12ca66910",
   "metadata": {},
   "source": [
    "## BaseOutputParser 를 구현한 OutputParser 만들어 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bf85a854-66f5-459d-acff-09e902a4e3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "이번예제에서는 LLM 의 출력을 → list 로 변환시켜 보자.\n",
    "LLM 의 출력(답변) 을 list 로 변환하는 OutputParser 를 만들어 보자\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aa392ad0-f81d-4bdb-8d55-95bc417386ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# v0.3\n",
    "from langchain_core.output_parsers.base import BaseOutputParser\n",
    "# https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.base.BaseOutputParser.html\n",
    "\n",
    "# ↓ 이를 상속 받아 OutputParser 를 만든다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0e60a193-a82e-455e-83cd-b85920d50333",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommaOutputParser(BaseOutputParser):\n",
    "\n",
    "    # parse() 를 반드시 구현해야 한다\n",
    "    #  text <- 입력텍스트\n",
    "    def parse(self, text):\n",
    "        items = text.strip().split(',')\n",
    "        return list(map(str.strip, items))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a095ce4f-eb65-4bcf-b5af-24de18d49631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 동작 확인\n",
    "p = CommaOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "65c6f2b4-e7da-491c-a0e7-5951b12697f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'how', 'are', 'you']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.parse(\"   Hello, how,    are,   you    \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0a5fe64c-8f9b-477a-a20e-546256d523f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Mercury\n",
      "2. Venus\n",
      "3. Earth\n",
      "4. Mars\n",
      "5. Jupiter\n",
      "6. Saturn\n",
      "7. Uranus\n",
      "8. Neptune\n",
      "9. Pluto\n"
     ]
    }
   ],
   "source": [
    "template = ChatPromptTemplate.from_messages([\n",
    "      (\"system\", \"\"\"You are a list generating machine.\n",
    "        Everything you are asked will be answered with a list of max {max_items}.\n",
    "        Do NOT reply with anything else.\"\"\"),\n",
    "\n",
    "      (\"human\", \"{question}\")\n",
    "  ])\n",
    "\n",
    "prompt = template.format_messages(\n",
    "    max_items=10,\n",
    "    question=\"What are the planets?\",\n",
    ")\n",
    "\n",
    "result = chat.invoke(prompt)\n",
    "\n",
    "print(result.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "982e9a58-68a9-42ba-9bdf-e6b2f58082e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune, Pluto\n"
     ]
    }
   ],
   "source": [
    "template = ChatPromptTemplate.from_messages([\n",
    "      (\"system\", \"\"\"You are a list generating machine.\n",
    "        Everything you are asked will be answered with a comma separated list of max {max_items}.\n",
    "        Do NOT reply with anything else.\"\"\"),\n",
    "\n",
    "      (\"human\", \"{question}\")\n",
    "  ])\n",
    "\n",
    "prompt = template.format_messages(\n",
    "    max_items=10,\n",
    "    question=\"What are the planets?\",\n",
    ")\n",
    "\n",
    "result = chat.invoke(prompt)\n",
    "\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c6555506-2c04-4239-a757-91ca633ba9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Red, blue, green, yellow, purple, orange, pink, black, white, brown\n"
     ]
    }
   ],
   "source": [
    "template = ChatPromptTemplate.from_messages([\n",
    "      (\"system\", \"\"\"You are a list generating machine.\n",
    "        Everything you are asked will be answered with a comma separated list of max {max_items}.\n",
    "        Do NOT reply with anything else.\"\"\"),\n",
    "\n",
    "      (\"human\", \"{question}\")\n",
    "  ])\n",
    "\n",
    "prompt = template.format_messages(\n",
    "    max_items=10,\n",
    "    question=\"What are the colors?\",\n",
    ")\n",
    "\n",
    "result = chat.invoke(prompt)\n",
    "\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2614be03-4af2-4718-b2a8-16cf10b8975a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "red, blue, green, yellow, pink, orange, purple, brown, black, white\n"
     ]
    }
   ],
   "source": [
    "template = ChatPromptTemplate.from_messages([\n",
    "      (\"system\", \"\"\"You are a list generating machine.\n",
    "        Everything you are asked will be answered \n",
    "        with a comma separated list of max {max_items} in lowercase.\n",
    "        Do NOT reply with anything else.\"\"\"),\n",
    "\n",
    "      (\"human\", \"{question}\")\n",
    "  ])\n",
    "\n",
    "prompt = template.format_messages(\n",
    "    max_items=10,\n",
    "    question=\"What are the colors?\",\n",
    ")\n",
    "\n",
    "result = chat.invoke(prompt)\n",
    "\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f8a96455-c744-421b-bd2e-ab2caa90e8ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['red',\n",
       " 'blue',\n",
       " 'green',\n",
       " 'yellow',\n",
       " 'orange',\n",
       " 'pink',\n",
       " 'purple',\n",
       " 'black',\n",
       " 'white',\n",
       " 'brown']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = ChatPromptTemplate.from_messages([\n",
    "      (\"system\", \"\"\"You are a list generating machine.\n",
    "        Everything you are asked will be answered \n",
    "        with a comma separated list of max {max_items} in lowercase.\n",
    "        Do NOT reply with anything else.\"\"\"),\n",
    "\n",
    "      (\"human\", \"{question}\")\n",
    "  ])\n",
    "\n",
    "prompt = template.format_messages(\n",
    "    max_items=10,\n",
    "    question=\"What are the colors?\",\n",
    ")\n",
    "\n",
    "result = chat.invoke(prompt)\n",
    "\n",
    "p = CommaOutputParser()\n",
    "p.parse(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6135b26d-7182-48f9-8cc6-beccd33273a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "template -> format -> invoke -> parser ...\n",
    "\n",
    "매 단계별로 하드코딩???\n",
    "\n",
    "↓ LCEL 을 사용하면 위 과정이 많~이 생략된다 => chain~!\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d393c45-617d-497a-bf61-6bc829545860",
   "metadata": {},
   "source": [
    "## Chain, LCEL\n",
    "\n",
    "- LCEL (LangChain Expression Language: 랭체인 표현 언어)\n",
    "  - LCEL은 LangChain 내에서 복잡한 표현식을 처리하고,\n",
    "  - 모델과의 상호작용을 더 강력하고 유연하게 만드는 기능을 제공\n",
    "    - 코드양을 많이 줄여줌.\n",
    "    - 다양한 template 과 LLM 호출\n",
    "    - 서로 다른 응답(response) 를 함께 사용케 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ac6f2f01-6b4a-4e60-8ac3-d24976bf5851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.runnables.base.RunnableSequence'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['max_items', 'question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['max_items'], input_types={}, partial_variables={}, template='You are a list generating machine.\\n        Everything you are asked will be answered \\n        with a comma separated list of max {max_items} in lowercase.\\n        Do NOT reply with anything else.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001A6F083BF80>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001A6F0E93AD0>, root_client=<openai.OpenAI object at 0x000001A6F04B7A40>, root_async_client=<openai.AsyncOpenAI object at 0x000001A6F0EB7D40>, model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "| CommaOutputParser()"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chain 생성\n",
    "#  '|' 연산자 사용\n",
    "#  LangChain 의 핵심!~\n",
    "\n",
    "chain = template | chat | CommaOutputParser()\n",
    "\n",
    "print(type(chain))\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "57f20fac-f3d6-4521-b8cb-6224494ed997",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bulbasaur', 'charmander', 'squirtle', 'pikachu', 'eevee']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chain 호출!  invoke({...})\n",
    "\n",
    "chain.invoke({\n",
    "    \"max_items\": 5,\n",
    "    \"question\": \"What are the pokemons?\",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7dee008-4130-4a74-a27d-49934b9193e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "['bulbasaur', 'charmander', 'squirtle', 'pikachu', 'eevee']\n",
    "\n",
    "↑ Chain 을 사용해 꽤나 간결한 코드로 작동된다!\n",
    "\n",
    "chain = template | chat | CommaOutputParser()\n",
    "\n",
    "사실 랭체인은 내부에서\n",
    "  .format_message() 호출 -> prompt 완성\n",
    "  -> chat.invoke() 호출 -> AIMessage 리턴\n",
    "  -> parse() 호출한다\n",
    "\n",
    "이러한 일련의 작업을 chain.invoke() 호출 단한번으로 끝낸다.\n",
    "\n",
    "이러한 chain 구문으로 정말 다양한 작업의 흐름들을 수행할수 있다.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8c41ad-67b6-464f-b0fa-a555e489f045",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "chain 끼리도 결합할수 도 있다.\n",
    "\n",
    "[예시]\n",
    "chain_one = template | chat | CommaOutputParser()\n",
    "chain_two = template_2 | chat | OutputParser2()\n",
    "\n",
    "all = chain_one | chain_two | OutputParser3()\n",
    "  ↑ chain_one 의 출력을 chain_two 의 입력값으로 사용 가능.\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe4839a-3039-4d92-8522-e92fd3ee2d9a",
   "metadata": {},
   "source": [
    "## Chaining Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a1d858-b1f1-4c26-aeb8-2575f98dfdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  공식]\n",
    "#  https://python.langchain.com/docs/concepts/lcel/\n",
    "#  https://python.langchain.com/docs/how_to/#langchain-expression-language-lcel\n",
    "#  https://python.langchain.com/docs/how_to/lcel_cheatsheet/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcacfe3-04e1-49ff-adc7-08219776e185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 랭체인 대신 OpenAI api 를 사용한다면 에서 gpt-3.5-turbo 에게서 응답 받는 방법은 대체로 다음과 같다.\n",
    "# https://platform.openai.com/docs/guides/text-generation\n",
    "\n",
    "\"\"\"\n",
    "# from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"developer\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Write a haiku about recursion in programming.\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)\n",
    "\"\"\"\n",
    "None\n",
    "\n",
    "# ↑물론 이는 openai 패키지를 사용하는 것이다\n",
    "\n",
    "# LangChain 내부에선 OpenAI python package 를 사용하곤 있지만,\n",
    "\n",
    "# LangChain 을 사용하지 않는다면 위와 같은 코드를 작성해야 한다는 것이다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ab113a-b945-47f5-b13a-cc40b918672c",
   "metadata": {},
   "source": [
    "## LCEL 의 input / output\n",
    "\n",
    "LangChain Expression Language (LCEL)은 LangChain에서 다양한 입력 유형을 활용하여 LLM과 도구를 결합하고 데이터 흐름을 제어하는 언어입니다. LCEL은 LLM의 입력과 처리에 사용되는 **입력 타입**(Input Types)을 명확하게 정의하여, 사용자 인터페이스와 도구 간의 상호작용을 더욱 효율적으로 만듭니다.\n",
    "\n",
    "아래는 LCEL에서 자주 사용되는 주요 **입력 타입**에 대한 설명입니다.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **Plain Text**\n",
    "- **설명**: 단순한 텍스트 입력입니다. 이 형식은 가장 기본적인 입력으로, LLM이 자유로운 자연어 처리를 수행할 수 있도록 합니다.\n",
    "- **예시**:\n",
    "  ```plaintext\n",
    "  What is the capital of France?\n",
    "  ```\n",
    "\n",
    "- **특징**:\n",
    "  - 텍스트 분석, 생성 및 대화형 작업에 적합.\n",
    "  - 추가적인 구조나 메타데이터 없이 단순 텍스트로 전달.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Structured Input**\n",
    "- **설명**: JSON, 딕셔너리, 또는 구조화된 형식의 입력입니다. 데이터 필드가 명시적으로 정의되어 있으며, 모델이 이 구조에 따라 데이터를 처리합니다.\n",
    "- **예시**:\n",
    "  ```json\n",
    "  {\n",
    "      \"question\": \"What is the capital of France?\",\n",
    "      \"context\": \"France is a country in Europe.\"\n",
    "  }\n",
    "  ```\n",
    "\n",
    "- **특징**:\n",
    "  - 명시적인 데이터 필드를 통해 LLM이 필요한 정보를 더 정확히 추출 및 활용 가능.\n",
    "  - 복잡한 데이터 분석이나 멀티 필드 처리가 필요한 작업에 유용.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Prompt Templates**\n",
    "- **설명**: 사용자가 정의한 프롬프트 템플릿을 입력으로 사용합니다. 템플릿에 변수 값을 채워 넣어 모델에 전달합니다.\n",
    "- **예시**:\n",
    "  ```python\n",
    "  template = \"Translate the following text to French: {text}\"\n",
    "  input = template.format(text=\"Hello, how are you?\")\n",
    "  ```\n",
    "\n",
    "- **특징**:\n",
    "  - 변수 기반 입력을 통해 재사용 가능성이 높음.\n",
    "  - 사용자 정의 입력 생성 및 제어에 적합.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Key-Value Pairs**\n",
    "- **설명**: 키-값 쌍의 입력 형식으로, 명시적인 쿼리 형태로 정보를 제공합니다.\n",
    "- **예시**:\n",
    "  ```python\n",
    "  {\n",
    "      \"name\": \"John\",\n",
    "      \"age\": 30,\n",
    "      \"location\": \"New York\"\n",
    "  }\n",
    "  ```\n",
    "\n",
    "- **특징**:\n",
    "  - 정형화된 데이터를 제공하여 LLM이 더 효율적으로 데이터를 분석 및 처리할 수 있음.\n",
    "  - 특정 정보 필드가 명확히 필요할 때 유용.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Multi-modal Inputs**\n",
    "- **설명**: 텍스트, 이미지, 오디오 등 다양한 데이터 유형을 조합한 입력 형식입니다.\n",
    "- **예시**:\n",
    "  ```python\n",
    "  {\n",
    "      \"text\": \"Describe the image.\",\n",
    "      \"image\": \"<image_data>\"\n",
    "  }\n",
    "  ```\n",
    "\n",
    "- **특징**:\n",
    "  - 멀티모달 모델과 통합하여 다양한 입력 형식을 처리 가능.\n",
    "  - 이미지 캡셔닝, 오디오-텍스트 변환 등의 작업에서 활용.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **Serialized Inputs**\n",
    "- **설명**: 입력 데이터를 시리얼화(Serialize)하여 특정 형식으로 변환한 입력입니다. 예를 들어, JSON 문자열로 데이터를 전달합니다.\n",
    "- **예시**:\n",
    "  ```python\n",
    "  input = '{\"question\": \"What is the capital of France?\", \"context\": \"France is in Europe.\"}'\n",
    "  ```\n",
    "\n",
    "- **특징**:\n",
    "  - 데이터가 외부 시스템이나 API와 통신할 때 유용.\n",
    "  - 데이터 포맷에 대한 유연성이 높음.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. **Chat Messages**\n",
    "- **설명**: 채팅 메시지 형식의 입력으로, 사용자가 역할(role)과 내용(content)을 정의하여 LLM에게 대화 형식으로 정보를 전달합니다.\n",
    "- **예시**:\n",
    "  ```python\n",
    "  [\n",
    "      {\"role\": \"system\", \"content\": \"You are an assistant.\"},\n",
    "      {\"role\": \"user\", \"content\": \"What is the weather today?\"}\n",
    "  ]\n",
    "  ```\n",
    "\n",
    "- **특징**:\n",
    "  - ChatGPT 같은 대화형 모델에 적합.\n",
    "  - 대화의 맥락을 유지하고 다중 발화 입력을 처리할 수 있음.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. **Custom Input Types**\n",
    "- **설명**: 사용자가 애플리케이션 요구 사항에 따라 정의하는 커스텀 입력 형식입니다.\n",
    "- **예시**:\n",
    "  ```python\n",
    "  class CustomInput:\n",
    "      def __init__(self, field1, field2):\n",
    "          self.field1 = field1\n",
    "          self.field2 = field2\n",
    "  ```\n",
    "\n",
    "- **특징**:\n",
    "  - 특정 애플리케이션 로직과 완벽히 맞는 형식으로 데이터 처리.\n",
    "  - 표준 입력 타입으로 표현하기 어려운 복잡한 구조를 다룰 때 유용.\n",
    "\n",
    "---\n",
    "\n",
    "### 요약\n",
    "LCEL의 입력 타입은 단순 텍스트부터 구조화된 데이터, 멀티모달 입력까지 다양하게 제공되며, 각 타입은 특정 용도에 맞게 설계되었습니다. 입력 데이터를 정교하게 설계하고 적절한 형식을 선택함으로써 모델의 성능을 최적화할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b323466-7086-495e-886e-0d7dca6abc1d",
   "metadata": {},
   "source": [
    "### 첫번째 chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fc649c7d-b2f5-4bcf-95b3-5e9b2f09837f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chef_prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', \n",
    "    \"\"\"\n",
    "      You are a world-class international chef.\n",
    "      You create easy to follow recipes for any type of cuisines\n",
    "      with easy to find ingredients.    \n",
    "    \"\"\"),\n",
    "    ('human', \n",
    "    \"\"\"\n",
    "        I want to cook {cuisine} food.\n",
    "    \"\"\"),\n",
    "])\n",
    "\n",
    "chef_chain = chef_prompt | chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a660d3-c18c-401d-a09b-7b35c87cf730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위 Chef 에게서 레시피를 받게 될텐데, 이게 첫번째 chain 의 출력결과다\n",
    "# 두번째 chain 에선 위 출력결과를 입력받아서 '채식 재료'만 사용하도록 변형 할겁니다.\n",
    "\n",
    "# 작업은 두개\n",
    "#   1. 레시피를 전달해주는 셰프\n",
    "#   2. 채식주의자를 위한 셰프"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ee78ab-f43f-45c8-bc8c-e4bd1373bd7d",
   "metadata": {},
   "source": [
    "### 두번째 chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "52d59cde-cd0f-4c68-873a-3f7e654960f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "veg_chef_prompt = ChatPromptTemplate.from_messages([\n",
    "  (\"system\",\n",
    "    \"\"\"\n",
    "      You are a vegetarian chef specialized on\n",
    "      making traditional recipies vegetarian.\n",
    "      You find alternative ingredients and explain their preparation.\n",
    "      You don't radically modify the recipe.\n",
    "      If there is no alternative for a food just say\n",
    "      you don't know how to replace it.\n",
    "    \"\"\"\n",
    "  ),\n",
    "  (\"human\",\"{recipe}\"),    \n",
    "])\n",
    "\n",
    "veg_chain = veg_chef_prompt | chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93185fe5-60f3-4cec-8304-201e3bec97e0",
   "metadata": {},
   "source": [
    "### final chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "885e9745-106e-499a-8873-65dd826618f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_chain = chef_chain | veg_chain\n",
    "\n",
    "# chef_chain 의 output 이 veg_chain 의 {recipe} 입력값으로 전달되게 하기\n",
    "final_chain = {'recipe': chef_chain} | veg_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4a759b2c-d8f2-4e99-8cec-038374a87582",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"For a vegetarian version of Chicken Tikka Masala, we can substitute the chicken with a plant-based alternative such as tofu or paneer. Here's how you can make Vegetarian Tikka Masala using paneer:\\n\\nIngredients:\\n- 1 lb paneer, cut into cubes\\n- 1 cup plain yogurt (you can use plant-based yogurt)\\n- 3 cloves garlic, minced\\n- 1-inch piece of ginger, grated\\n- 1 tbsp garam masala\\n- 1 tsp ground turmeric\\n- 1 tsp ground cumin\\n- 1 tsp ground coriander\\n- 1/2 tsp chili powder (adjust to taste)\\n- Salt and pepper to taste\\n- 2 tbsp vegetable oil\\n- 1 onion, finely chopped\\n- 1 can (14 oz) diced tomatoes\\n- 1 cup heavy cream or coconut cream\\n- Fresh cilantro, chopped (for garnish)\\n- Cooked basmati rice or naan (to serve)\\n\\nInstructions:\\n1. Instead of marinating chicken, marinate the paneer cubes in a mixture of yogurt, garlic, ginger, garam masala, turmeric, cumin, coriander, chili powder, salt, and pepper. Cover and refrigerate for at least 1 hour.\\n\\n2. Follow the same steps as the original recipe, but when cooking, be gentle with the paneer to prevent it from breaking. Cook until the paneer is heated through and has absorbed the flavors of the sauce.\\n\\n3. Serve the Vegetarian Tikka Masala over cooked basmati rice or with warm naan. Garnish with chopped cilantro.\\n\\nEnjoy your flavorful Vegetarian Tikka Masala! You can still adjust the spice levels to your liking and pair it with cucumber raita or mango chutney for a delightful meal.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 375, 'prompt_tokens': 826, 'total_tokens': 1201, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-Bor15ot2tsCSQmNn8VtWkgn3fY4dH', 'finish_reason': 'stop', 'logprobs': None}, id='run--61008fed-585d-4ff6-88cc-7b2460a3b7d1-0', usage_metadata={'input_tokens': 826, 'output_tokens': 375, 'total_tokens': 1201, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = final_chain.invoke({\n",
    "    'cuisine': 'indian',   # 첫번째 chain 인 chef_chain 의 {cuisine} 에 전달\n",
    "})\n",
    "\n",
    "# ↓ 두번째 chain 인 veg_chain 의 결과.\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1d540e1e-9547-4359-8841-bab23961afed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For a vegetarian version of Chicken Tikka Masala, we can substitute the chicken with a plant-based alternative such as tofu or paneer. Here's how you can make Vegetarian Tikka Masala using paneer:\n",
      "\n",
      "Ingredients:\n",
      "- 1 lb paneer, cut into cubes\n",
      "- 1 cup plain yogurt (you can use plant-based yogurt)\n",
      "- 3 cloves garlic, minced\n",
      "- 1-inch piece of ginger, grated\n",
      "- 1 tbsp garam masala\n",
      "- 1 tsp ground turmeric\n",
      "- 1 tsp ground cumin\n",
      "- 1 tsp ground coriander\n",
      "- 1/2 tsp chili powder (adjust to taste)\n",
      "- Salt and pepper to taste\n",
      "- 2 tbsp vegetable oil\n",
      "- 1 onion, finely chopped\n",
      "- 1 can (14 oz) diced tomatoes\n",
      "- 1 cup heavy cream or coconut cream\n",
      "- Fresh cilantro, chopped (for garnish)\n",
      "- Cooked basmati rice or naan (to serve)\n",
      "\n",
      "Instructions:\n",
      "1. Instead of marinating chicken, marinate the paneer cubes in a mixture of yogurt, garlic, ginger, garam masala, turmeric, cumin, coriander, chili powder, salt, and pepper. Cover and refrigerate for at least 1 hour.\n",
      "\n",
      "2. Follow the same steps as the original recipe, but when cooking, be gentle with the paneer to prevent it from breaking. Cook until the paneer is heated through and has absorbed the flavors of the sauce.\n",
      "\n",
      "3. Serve the Vegetarian Tikka Masala over cooked basmati rice or with warm naan. Garnish with chopped cilantro.\n",
      "\n",
      "Enjoy your flavorful Vegetarian Tikka Masala! You can still adjust the spice levels to your liking and pair it with cucumber raita or mango chutney for a delightful meal.\n"
     ]
    }
   ],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ced63d-336c-4d8a-9a00-9fdb42c73178",
   "metadata": {},
   "source": [
    "## streaming= 과 callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11852694-456c-45c5-99f1-02a7567eafbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ↑ 전부 실행 완료 될때까지 기다리는게 지루하다.\n",
    "#   어떻게 진행되는지도 궁금하다.\n",
    "#   진행되는 과정을 실시간으로 출력 할수 있다!\n",
    "\n",
    "# Chat model 의 streaming=\n",
    "#  streaming 은 LLM model 의 응답(resposne) 이 생성되는 것을\n",
    "#    실시간으로(?) 보게 해줌.\n",
    "\n",
    "# callbacks=[StreamingStdOutCallbackHandler()]\n",
    "#    볼수 있는 문자(토큰)가 생길 때마다 print 해준다.\n",
    "\n",
    "# callbacks 는 다양한 'event' 감지도 가능\n",
    "#    LLM 이 작업을 시작했다거나, 끝냈다거나.\n",
    "#    문자를 생성했다거나, 에러가 발생하거나..\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "02ff69f9-ab7a-453c-9d15-7d7dc5dd8e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# v0.3    https://python.langchain.com/api_reference/core/callbacks/langchain_core.callbacks.streaming_stdout.StreamingStdOutCallbackHandler.html\n",
    "from langchain_core.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a80db619-3b91-4771-87ca-07c7ea6f18d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "aed8a15f-5ca9-4365-93f8-2f6aaa6d6ea5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great choice! Indian cuisine is full of delicious flavors and spices. Let's make a classic dish - Chicken Tikka Masala. Here's a simple recipe for you to try at home:\n",
      "\n",
      "Ingredients:\n",
      "- 1 lb boneless, skinless chicken breasts, cut into bite-sized pieces\n",
      "- 1 cup plain yogurt\n",
      "- 2 tablespoons lemon juice\n",
      "- 2 teaspoons ground cumin\n",
      "- 2 teaspoons paprika\n",
      "- 1 teaspoon ground cinnamon\n",
      "- 1 teaspoon ground turmeric\n",
      "- 1 teaspoon ground coriander\n",
      "- 1 teaspoon cayenne pepper (adjust to taste)\n",
      "- Salt and pepper to taste\n",
      "- 2 tablespoons vegetable oil\n",
      "- 1 onion, finely chopped\n",
      "- 3 cloves garlic, minced\n",
      "- 1 tablespoon grated ginger\n",
      "- 1 can (14 oz) tomato sauce\n",
      "- 1 cup heavy cream\n",
      "- Fresh cilantro, chopped (for garnish)\n",
      "- Cooked rice or naan bread (for serving)\n",
      "\n",
      "Instructions:\n",
      "1. In a bowl, combine yogurt, lemon juice, cumin, paprika, cinnamon, turmeric, coriander, cayenne pepper, salt, and pepper. Add the chicken pieces and coat them well with the marinade. Cover and refrigerate for at least 1 hour, or overnight for best results.\n",
      "\n",
      "2. Preheat the oven to 400°F (200°C). Thread the marinated chicken onto skewers and place them on a baking sheet. Bake for 20-25 minutes or until the chicken is cooked through.\n",
      "\n",
      "3. In a large skillet, heat vegetable oil over medium heat. Add the chopped onion and cook until softened, about 5 minutes. Add the garlic and ginger, and cook for another minute.\n",
      "\n",
      "4. Stir in the tomato sauce and bring to a simmer. Add the baked chicken pieces to the sauce and simmer for 10 minutes.\n",
      "\n",
      "5. Stir in the heavy cream and simmer for an additional 5 minutes. Adjust seasoning with salt and pepper if needed.\n",
      "\n",
      "6. Serve the Chicken Tikka Masala over cooked rice or with naan bread. Garnish with chopped cilantro.\n",
      "\n",
      "Enjoy your homemade Chicken Tikka Masala! Feel free to adjust the spice levels to suit your taste preferences.For a vegetarian version of Chicken Tikka Masala, we can replace the chicken with a plant-based alternative like tofu or paneer. Here's how you can adapt the recipe:\n",
      "\n",
      "Ingredients:\n",
      "- 1 lb firm tofu or paneer, cut into bite-sized pieces\n",
      "- 1 cup plain yogurt (you can use dairy-free yogurt for a vegan option)\n",
      "- 2 tablespoons lemon juice\n",
      "- 2 teaspoons ground cumin\n",
      "- 2 teaspoons paprika\n",
      "- 1 teaspoon ground cinnamon\n",
      "- 1 teaspoon ground turmeric\n",
      "- 1 teaspoon ground coriander\n",
      "- 1 teaspoon cayenne pepper (adjust to taste)\n",
      "- Salt and pepper to taste\n",
      "- 2 tablespoons vegetable oil\n",
      "- 1 onion, finely chopped\n",
      "- 3 cloves garlic, minced\n",
      "- 1 tablespoon grated ginger\n",
      "- 1 can (14 oz) tomato sauce\n",
      "- 1 cup coconut cream (or another dairy-free alternative)\n",
      "- Fresh cilantro, chopped (for garnish)\n",
      "- Cooked rice or naan bread (for serving)\n",
      "\n",
      "Instructions:\n",
      "1. Follow the same marinating process as the original recipe but use tofu or paneer instead of chicken. Marinate the tofu or paneer in the yogurt and spice mixture for at least 1 hour.\n",
      "\n",
      "2. Instead of baking the chicken, you can pan-fry the marinated tofu or paneer until golden brown on all sides.\n",
      "\n",
      "3. Proceed with the recipe as instructed, replacing the chicken with the cooked tofu or paneer when adding it to the tomato sauce.\n",
      "\n",
      "4. Substitute heavy cream with coconut cream or another dairy-free alternative to maintain the creamy texture of the dish.\n",
      "\n",
      "5. Adjust the seasoning as needed and serve the Vegetarian Tikka Masala over rice or with naan bread. Garnish with chopped cilantro before serving.\n",
      "\n",
      "Enjoy your flavorful Vegetarian Tikka Masala! It's a delicious meat-free alternative to the classic dish.For a vegetarian version of Chicken Tikka Masala, we can replace the chicken with a plant-based alternative like tofu or paneer. Here's how you can adapt the recipe:\n",
      "\n",
      "Ingredients:\n",
      "- 1 lb firm tofu or paneer, cut into bite-sized pieces\n",
      "- 1 cup plain yogurt (you can use dairy-free yogurt for a vegan option)\n",
      "- 2 tablespoons lemon juice\n",
      "- 2 teaspoons ground cumin\n",
      "- 2 teaspoons paprika\n",
      "- 1 teaspoon ground cinnamon\n",
      "- 1 teaspoon ground turmeric\n",
      "- 1 teaspoon ground coriander\n",
      "- 1 teaspoon cayenne pepper (adjust to taste)\n",
      "- Salt and pepper to taste\n",
      "- 2 tablespoons vegetable oil\n",
      "- 1 onion, finely chopped\n",
      "- 3 cloves garlic, minced\n",
      "- 1 tablespoon grated ginger\n",
      "- 1 can (14 oz) tomato sauce\n",
      "- 1 cup coconut cream (or another dairy-free alternative)\n",
      "- Fresh cilantro, chopped (for garnish)\n",
      "- Cooked rice or naan bread (for serving)\n",
      "\n",
      "Instructions:\n",
      "1. Follow the same marinating process as the original recipe but use tofu or paneer instead of chicken. Marinate the tofu or paneer in the yogurt and spice mixture for at least 1 hour.\n",
      "\n",
      "2. Instead of baking the chicken, you can pan-fry the marinated tofu or paneer until golden brown on all sides.\n",
      "\n",
      "3. Proceed with the recipe as instructed, replacing the chicken with the cooked tofu or paneer when adding it to the tomato sauce.\n",
      "\n",
      "4. Substitute heavy cream with coconut cream or another dairy-free alternative to maintain the creamy texture of the dish.\n",
      "\n",
      "5. Adjust the seasoning as needed and serve the Vegetarian Tikka Masala over rice or with naan bread. Garnish with chopped cilantro before serving.\n",
      "\n",
      "Enjoy your flavorful Vegetarian Tikka Masala! It's a delicious meat-free alternative to the classic dish.\n"
     ]
    }
   ],
   "source": [
    "# 위 Chat model 로 다시 실행해보기\n",
    "chef_chain = chef_prompt | chat\n",
    "veg_chain = veg_chef_prompt | chat\n",
    "final_chain = {\"recipe\": chef_chain} | veg_chain\n",
    "\n",
    "result = final_chain.invoke({\n",
    "    \"cuisine\": \"indian\",  # chef_chain 의 {cuisine} 에 전달\n",
    "})\n",
    "\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47887fb9-d1a5-48da-857b-2f148f5eba4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7c92f3-e264-4ba8-bf43-a9eef097553f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034a140f-e827-4025-9e22-4ca384761412",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719e37c6-58a4-4bf8-81ae-f60402ad555a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c4559b-620c-4878-9afa-1a7b9b4f2f60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72666220-fdce-4d6a-84a7-8e87fa008ca2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f11f28-79a9-44c5-9a53-be35923ab78b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7aa87b-e9bf-4eaa-a29f-288e00914144",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abdcc7e-7d09-425f-8764-c1d5b21ab5ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a58c92-d695-4964-b796-9f6c785cae4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fd1613-66c6-4419-bcf2-f64b4f69ac67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca073702-ad05-4e24-827c-50ec33d90230",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1aba4b1-dbcc-4d83-8f1b-8e7adfda8e93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3630ff2b-2c6e-4a30-bde4-f368eed2510b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3d61ac-8ea6-4939-9f7a-9c26829dec62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8704e24e-0f22-4c0e-b6ff-bd2e15f074f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20460a92-c623-4eb7-8a9f-c0a67555b543",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb02ab7-20fb-4a93-be59-8bae539ea039",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94786266-aa5d-48b8-b8ae-e3f93e20fcb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fc3d96-1a98-42c1-9f35-23904d28e34e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61698108-0d18-449f-a950-f9159632d3b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
