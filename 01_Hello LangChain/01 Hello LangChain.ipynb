{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d56785dc-8934-428d-ae51-82aba42a2838",
   "metadata": {},
   "source": [
    "# LLM and ChatModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d948f23d-1476-4e94-b197-725a16e5d72a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# LangChain  ê´€ë ¨ ì£¼ìš” ë§í¬\n",
    "\n",
    "-  Python Langchain ê³µì‹ í™ˆ:  https://python.langchain.com/\n",
    "-  API ë ˆí¼ëŸ°ìŠ¤ í™ˆ: https://python.langchain.com/api_reference/reference.html\n",
    "\n",
    "\n",
    "## Langchain ì˜ íŒ¨í‚¤ì§€ êµ¬ì„±\n",
    "\n",
    "\n",
    "### Base Packages\n",
    "- [Core: langchain-core](https://python.langchain.com/api_reference/core)\n",
    "- [Langchain: langchain](https://python.langchain.com/api_reference/langchain)\n",
    "- [Test Splitters: langchain-text-splitters](https://python.langchain.com/api_reference/text_splitters)\n",
    "- [Community: langchain-community](https://python.langchain.com/api_reference/community)\n",
    "- [Experimental: langchain-experimental](https://python.langchain.com/api_reference/experimental)\n",
    "\n",
    "### Integrations\n",
    "- ë­ì²´ì¸ì€ ìˆ˜ë§ì€ LLM ëª¨ë¸ë“¤ê³¼ ì»¤ë®¤ë‹ˆí‹°, ë²¡í„°ìŠ¤í† ì–´, ë°ì´í„°ë² ì´ìŠ¤, íˆ´ ë“¤ê³¼ í•¨ê»˜ ì‚¬ìš©í• ìˆ˜ ìˆë„ë¡ ì œê³µë˜ëŠ” íŒ¨í‚¤ì§€ë“¤ì´ ë§ë‹¤ (ì•ìœ¼ë¡œ ë” ë§ì•„ ì§ˆê±°ë‹¤)\n",
    "- [OpanAI: langchain-openai](https://python.langchain.com/api_reference/openai)\n",
    "- [Huggingface: langchain-huggingface](https://python.langchain.com/api_reference/huggingface)\n",
    "- [MistalAI: langchain-mistralai](https://python.langchain.com/api_reference/mistralai)\n",
    "- ê·¸ë°–ì—ë„ ë§ì´ ìˆë‹¤ ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc357e1b-6e09-455b-9328-e3574203d48e",
   "metadata": {},
   "source": [
    "# í™˜ê²½ë³€ìˆ˜ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b55641f-c5df-4bd5-a966-2f984e682f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa33b18d-096b-4ed1-b65a-eecadd8bbe7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sk-proj-iKU13YeoxNgF...\n"
     ]
    }
   ],
   "source": [
    "print(f'{os.environ['OPENAI_API_KEY'][:20]}...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e7469d-dbc9-4a56-bc79-49143407370b",
   "metadata": {},
   "source": [
    "# â–  LLM vs. Chat model\n",
    "\n",
    "LangChain ì€ LLM ê³¼ Chat model ë‘ê°€ì§€ë¥¼ ì§€ì›í•©ë‹ˆë‹¤\n",
    "\n",
    "`LLM`(Large Language Model)ê³¼ `Chat Model`ì€ ë¹„ìŠ·í•œ ì—­í• ì„ í•˜ì§€ë§Œ, ì•½ê°„ì˜ ì°¨ì´ì ì´ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì´ëŠ” ì£¼ë¡œ **ëª¨ë¸ì˜ ì…ë ¥ ë° ìƒí˜¸ì‘ìš© ë°©ì‹**ì—ì„œ ë‚˜íƒ€ë‚œë‹¤.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. LLM (Large Language Model)\n",
    "- **íŠ¹ì§•**:\n",
    "  - ì¼ë°˜ì ìœ¼ë¡œ **í…ìŠ¤íŠ¸ ì…ë ¥**ì„ ë°›ê³ , ì´ì— ëŒ€í•œ í…ìŠ¤íŠ¸ ì¶œë ¥ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "  - ë‹¨ìˆœí•œ í”„ë¡¬í”„íŠ¸ ê¸°ë°˜ ì…ë ¥/ì¶œë ¥ì„ ì²˜ë¦¬í•˜ê¸° ìœ„í•œ ëª¨ë¸ì…ë‹ˆë‹¤.\n",
    "  - ì‚¬ìš©ìê°€ ì œê³µí•œ ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ê³ , ê·¸ì— ëŒ€í•œ ê²°ê³¼ë¥¼ í•œ ë²ˆì— ì¶œë ¥í•©ë‹ˆë‹¤.\n",
    "- **ì…ë ¥ í˜•ì‹**:\n",
    "  ```plaintext\n",
    "  \"Tell me a summary of the benefits of LangChain.\"\n",
    "  ```\n",
    "- **ì¶œë ¥ í˜•ì‹**:\n",
    "  ```plaintext\n",
    "  \"LangChain is a framework designed to simplify the development of applications powered by large language models, making it easier to manage prompts, chains, and integrations.\"\n",
    "  ```\n",
    "- **ì£¼ìš” ì‚¬ìš© ì‚¬ë¡€**:\n",
    "  - ë‹¨ì¼ ì§ˆë¬¸-ë‹µë³€\n",
    "  - í…ìŠ¤íŠ¸ ìƒì„±\n",
    "  - ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì‘ì—…\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Chat Model\n",
    "- **íŠ¹ì§•**:\n",
    "  - **ëŒ€í™” í˜•ì‹**ìœ¼ë¡œ ì„¤ê³„ëœ ëª¨ë¸ë¡œ, ë‹¤ì¤‘ í„´ ëŒ€í™”ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆë‹¤.\n",
    "  - ì…ë ¥ í˜•ì‹ì´ **ë©”ì‹œì§€ Message**ë¡œ êµ¬ì„±ë˜ë©°, ê° ë©”ì‹œì§€ëŠ” ì‚¬ìš©ìì˜ ë©”ì‹œì§€ (User Message)ì™€ ì‹œìŠ¤í…œì˜ ë©”ì‹œì§€(System Message)ë¡œ ë‚˜ë‰œë‹¤.\n",
    "  - 'ë¬¸ë§¥'ì„ ì´í•´í•˜ê³  'ëŒ€í™”ì˜ íë¦„'ì„ ìœ ì§€í•˜ëŠ” ë° ìµœì í™”ë˜ì–´ ìˆë‹¤.\n",
    "- **ì…ë ¥ í˜•ì‹**:\n",
    "  ë©”ì‹œì§€ ê°ì²´ë¥¼ ì „ë‹¬í•´ì•¼ í•˜ë©°, ë³´í†µ ì•„ë˜ì™€ ê°™ì€ êµ¬ì¡°ì…ë‹ˆë‹¤.\n",
    "  ```python\n",
    "  [\n",
    "      {\"role\": \"system\", \"content\": \"You are an assistant who helps with Python programming.\"},\n",
    "      {\"role\": \"user\", \"content\": \"Can you explain the difference between LLM and chat models in LangChain?\"}\n",
    "  ]\n",
    "  ```\n",
    "- **ì¶œë ¥ í˜•ì‹**:\n",
    "  ```python\n",
    "  {\"role\": \"assistant\", \"content\": \"Sure! LLM and Chat Models differ in their input and interaction styles...\"}\n",
    "  ```\n",
    "- **ì£¼ìš” ì‚¬ìš© ì‚¬ë¡€**:\n",
    "  - ë‹¤ì¤‘ í„´ ëŒ€í™”\n",
    "  - ë¬¸ë§¥ ì¶”ì  ë° ìœ ì§€ (ëŒ€í™” íˆìŠ¤í† ë¦¬ ë°˜ì˜)\n",
    "  - ëŒ€í™” ê¸°ë°˜ ì±—ë´‡, FAQ ì‹œìŠ¤í…œ ë“±\n",
    "\n",
    "---\n",
    "\n",
    "### 3. ì£¼ìš” ì°¨ì´ì  ìš”ì•½\n",
    "| **íŠ¹ì§•**        | **LLM**                                                | **Chat Model**                                        |\n",
    "|-----------------|------------------------------------------------------|----------------------------------------------------|\n",
    "| **ì…ë ¥ í˜•ì‹**   | ë‹¨ì¼ í…ìŠ¤íŠ¸ ì…ë ¥                                         | ì—­í•  ê¸°ë°˜ì˜ ëŒ€í™” ë©”ì‹œì§€ ê°ì²´ (role: system, user, assistant) |\n",
    "| **ëŒ€í™” íˆìŠ¤í† ë¦¬**| ë¬¸ë§¥ ì¶”ì  ë¶ˆê°€ëŠ¥ (ë‹¨ì¼ ìš”ì²­ ì²˜ë¦¬)                          | ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ í†µí•´ ë¬¸ë§¥ì„ ìœ ì§€í•˜ê³  ë°˜ì˜                 |\n",
    "| **ì‚¬ìš© ëª©ì **   | í…ìŠ¤íŠ¸ ìƒì„±, ìš”ì•½, ë‹¨ìˆœ ì§ˆì˜ì‘ë‹µ                             | ëŒ€í™”í˜• ì¸í„°í˜ì´ìŠ¤, ì±—ë´‡, ë‹¤ì¤‘ í„´ ì§ˆì˜ì‘ë‹µ               |\n",
    "| **ì‘ìš© ì‚¬ë¡€**   | ë‹¨ì¼ ì§ˆë¬¸-ë‹µë³€, í…ìŠ¤íŠ¸ ìƒì„±                                | ê³ ê° ì§€ì› ì±—ë´‡, ì¸í„°ë™í‹°ë¸Œ Q&A, ë©€í‹°í„´ ëŒ€í™” ì‹œìŠ¤í…œ          |\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### 5. ì–¸ì œ ì–´ë–¤ ê²ƒì„ ì„ íƒí•´ì•¼ í• ê¹Œìš”?\n",
    "- **ë‹¨ì¼ ì‘ì—…ì´ë‚˜ ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ìƒì„±**:\n",
    "  - `LLM`ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì í•©í•©ë‹ˆë‹¤.\n",
    "- **ëŒ€í™” ê¸°ë°˜ ì• í”Œë¦¬ì¼€ì´ì…˜ì´ë‚˜ ë¬¸ë§¥ì„ ìœ ì§€í•´ì•¼ í•˜ëŠ” ì‘ì—…**:\n",
    "  - `Chat Model`ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë” ì í•©í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11badd6-ebc2-4a45-b1d4-34e9fa44090e",
   "metadata": {},
   "source": [
    "# LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a772a3b-6e39-471d-bc50-28fa86508ad1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.3.23'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import langchain\n",
    "langchain.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9545aef-1c32-476e-ba41-f7559d5f61d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM ëª¨ë¸\n",
    "from langchain_openai.llms.base import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e6099f2-0777-46c2-9401-1816c849eb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatModel\n",
    "from langchain_openai.chat_models.base import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21267cbd-3f8e-4e39-90d4-f9aa1c37ffcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI()   # OPENAI_API_KEY í•„ìš”!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6bece8b-3f39-498e-9636-85f53d6f50f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gpt-3.5-turbo-instruct'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7e6bd7e-502e-4cee-a744-399b5aee7574",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f481cc31-e087-4f1a-897f-f165fc29f0f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gpt-3.5-turbo'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef419717-f2b1-45f1-9ea9-b1ff3ea669ae",
   "metadata": {},
   "source": [
    "# LLM í˜¸ì¶œ (invoke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "736ad35f-d6ad-4deb-b8e3-bd0355d3b446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "ë‹µë³€ \n",
      "\n",
      "There are currently eight planets in our solar system: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and Neptune. However, there may be more planets beyond our solar system that have not yet been discovered. \n"
     ]
    }
   ],
   "source": [
    "result = llm.invoke(\"How many planets are there?\")  # ì…ë ¥ str\n",
    "print(type(result))  # ì¶œë ¥ str\n",
    "print('ë‹µë³€', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eccd5eb9-6df0-4e38-b0a1-e7ec69b2aa61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "ë‹µë³€ \n",
      "íƒœì–‘ê³„ì—ëŠ” 8ê°œì˜ í–‰ì„±ì´ ìˆìŠµë‹ˆë‹¤. ì´ë“¤ì€ ìˆ˜ì„±, ê¸ˆì„±, ì§€êµ¬, í™”ì„±, ëª©ì„±, í† ì„±, ì²œì™•ì„±, í•´ì™•ì„±ì…ë‹ˆë‹¤. í•˜ì§€ë§Œ ìµœê·¼ì—ëŠ” ëª…ì™•ì„±ì„ ë¹„ë¡¯í•œ ì—¬ëŸ¬ ê°œì˜ ì†Œí–‰ì„±ê³¼ ë¯¸ì„¸ í–‰ì„±ë“¤ë„ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "result = llm.invoke(\"íƒœì–‘ê³„ì—ëŠ” ì–¼ë§ˆë‚˜ ë§ì€ í–‰ì„±ë“¤ì´ ìˆì£ ?\")  # ì…ë ¥ str\n",
    "print(type(result))  # ì¶œë ¥ str\n",
    "print('ë‹µë³€', result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5e9794-de7f-493b-b44f-f4f04c801242",
   "metadata": {},
   "source": [
    "# ChatModel í˜¸ì¶œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5df10da-615a-4575-902f-3b7bff22401a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "ë‹µë³€ content='In our solar system, there are 8 planets: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and Neptune.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 29, 'prompt_tokens': 13, 'total_tokens': 42, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BopJL7k9QhssZYWnoHL9xwKTP6L0e', 'finish_reason': 'stop', 'logprobs': None} id='run--78a43e8a-5c6b-432d-b4b7-c4a337347e7f-0' usage_metadata={'input_tokens': 13, 'output_tokens': 29, 'total_tokens': 42, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "ğŸ’™ In our solar system, there are 8 planets: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and Neptune.\n"
     ]
    }
   ],
   "source": [
    "result = chat.invoke(\"How many planets are there?\")  # ì…ë ¥ str\n",
    "print(type(result))  # ì¶œë ¥ Message\n",
    "print('ë‹µë³€', result)\n",
    "print('ğŸ’™', result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb365338-1961-4781-a8ae-dadc9464eaf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "ë‹µë³€ content='íƒœì–‘ê³„ì—ëŠ” ì´ 8ê°œì˜ í–‰ì„±ì´ ìˆìŠµë‹ˆë‹¤. ìˆ˜ì„±, ê¸ˆì„±, ì§€êµ¬, í™”ì„±, ëª©ì„±, í† ì„±, ì²œì™•ì„±, ëª…ì™•ì„±ì…ë‹ˆë‹¤.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 57, 'prompt_tokens': 32, 'total_tokens': 89, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BopVwvkGbPyWN0w4uqcZGdPZKfVmZ', 'finish_reason': 'stop', 'logprobs': None} id='run--4dbac12d-a982-480c-b4d1-075e7d1c78d2-0' usage_metadata={'input_tokens': 32, 'output_tokens': 57, 'total_tokens': 89, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "ğŸ’™ íƒœì–‘ê³„ì—ëŠ” ì´ 8ê°œì˜ í–‰ì„±ì´ ìˆìŠµë‹ˆë‹¤. ìˆ˜ì„±, ê¸ˆì„±, ì§€êµ¬, í™”ì„±, ëª©ì„±, í† ì„±, ì²œì™•ì„±, ëª…ì™•ì„±ì…ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "result = chat.invoke(\"íƒœì–‘ê³„ì—ëŠ” ì–¼ë§ˆë‚˜ ë§ì€ í–‰ì„±ë“¤ì´ ìˆì£ ?\")  # ì…ë ¥ str\n",
    "print(type(result))  # ì¶œë ¥ Message\n",
    "print('ë‹µë³€', result)\n",
    "print('ğŸ’™', result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0321dc54-802e-4978-9442-877617858144",
   "metadata": {},
   "source": [
    "## í•œê¸€ or ì˜ì–´ ?\n",
    "\n",
    "ì±— GPTì˜ ì–¸ì–´ ì²˜ë¦¬ ëŠ¥ë ¥ì€ ì¸ê³µì§€ëŠ¥ ê¸°ìˆ ì˜ í›Œë¥­í•œ ë°œì „ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. í•˜ì§€ë§Œ ì‚¬ìš©ìê°€ ë°›ëŠ” ë‹µë³€ì˜ í’ˆì§ˆì€ ì œì¶œí•˜ëŠ” ì–¸ì–´ì— ë”°ë¼ ì•½ê°„ì˜ ì°¨ì´ê°€ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŸ° ì°¨ì´ëŠ” ì±— GPTê°€ í•™ìŠµí•˜ëŠ” ê³¼ì •ì—ì„œ ë‹¤ì–‘í•œ ì–¸ì–´ì˜ ë°ì´í„° ì–‘ê³¼ í’ˆì§ˆ, ê·¸ë¦¬ê³  ì–¸ì–´ë³„ íŠ¹ì„±ì„ ì–¼ë§ˆë‚˜ ì˜ ì²˜ë¦¬í•˜ëŠ”ì§€ì— ë”°ë¼ ê²°ì •ë©ë‹ˆë‹¤.\n",
    "\n",
    "OpenAIì˜ ì–¸ì–´ ëª¨ë¸, íŠ¹íˆ GPT ì‹œë¦¬ì¦ˆëŠ” ë‹¤ì–‘í•œ ë°ì´í„° ì†ŒìŠ¤ì—ì„œ ì–»ì€ ëŒ€ëŸ‰ì˜ ë°ì´í„°ë¡œ í•™ìŠµë©ë‹ˆë‹¤. ì´ ë°ì´í„°ëŠ” ì£¼ë¡œ ì˜ì–´ë¥¼ ë¹„ë¡¯í•œ ì—¬ëŸ¬ ì–¸ì–´ì—ì„œ ìˆ˜ì§‘ë˜ë©°, í•™ìŠµ ë°ì´í„°ì˜ êµ¬ì„±ì€ ëª¨ë¸ì˜ ì„±ëŠ¥ê³¼ ì¼ë°˜í™” ëŠ¥ë ¥ì— í° ì˜í–¥ì„ ë¯¸ì¹©ë‹ˆë‹¤.\n",
    "\n",
    "ì˜ì–´ëŠ” ì „ì„¸ê³„ì ìœ¼ë¡œ ë§ì´ ì‚¬ìš©ë˜ë©°*, ì¸í„°ë„· ìƒì˜ ë°ì´í„°ë„ ì˜ì–´ê°€ ë§ì•„ì„œ ì±— GPTëŠ” ì˜ì–´ ì§ˆë¬¸ì— ëŒ€í•´ ë” ì •í™•í•˜ê³  ìì—°ìŠ¤ëŸ¬ìš´ ë‹µë³€ì„ ì œê³µí•  í™•ë¥ ì´ ë†’ìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ í•œêµ­ì–´ì™€ ê°™ì€ ë‹¤ë¥¸ ì–¸ì–´ëŠ” ìƒëŒ€ì ìœ¼ë¡œ ë°ì´í„°ê°€ ë¶€ì¡±í•˜ê±°ë‚˜, ì–¸ì–´ì˜ ë³µì¡ì„± ë•Œë¬¸ì— ì²˜ë¦¬ê°€ ë” ì–´ë ¤ìš¸ ìˆ˜ ìˆì–´, ì´ë¡œ ì¸í•´ ë‹µë³€ì˜ í’ˆì§ˆì— ì°¨ì´ê°€ ë‚˜íƒ€ë‚  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì°¸ê³ \n",
    "- https://fastcampus.co.kr/gov_review_insightGPTlang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1853dab-e915-44a7-8cfe-bbd44f4a8fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì•„ë˜ì™€ ê°™ì´ api í‚¤ë¥¼ ì§ì ‘ ë§¤ê°œë³€ìˆ˜ë¡œ ê±´ë„¤ì¤„ìˆ˜ë„ ìˆì§€ë§Œ...  KEY ë¹„ì¶”í•œë‹¤.  í™˜ê²½ë³€ìˆ˜ ì‚¬ìš©ì„ ì¶”ì²œí•œë‹¤.\n",
    "#\n",
    "# llm = OpenAI(openai_api_key=\"sk-\")\n",
    "# chat = ChatOpenAI(openai_api_key=\"sk-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "201fe7e7-e08a-49df-b90b-d51d6ce68b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic.chat_models import ChatAnthropic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3adea02e-2971-46e8-b690-f87fc19c676d",
   "metadata": {},
   "source": [
    "# Invoke Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "90adc0eb-e797-477e-a832-51f4c38fa691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatModel ì€ 'ì§ˆë¬¸'ë§Œ ë°›ëŠ”ê²Œ ì•„ë‹ˆë¼ 'ëŒ€í™”' ë„ í• ìˆ˜ ìˆë‹¤ (Message ë¥¼ ë³´ë‚¼ìˆ˜ë„ ìˆë‹¤)\n",
    "# 'ëŒ€í™”(conversation)' ì€\n",
    "#    : ì—¬ëŸ¬ ë©”ì„¸ì§€ ë¬¶ìŒ\n",
    "#    : ìƒëŒ€ì˜ ëŒ€í™”ì˜ ë§¥ë½ì— ë§ê²Œ ëŒ€ë‹µí• ìˆ˜ ìˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccef9399-4db3-4d1b-93bb-e1b6e61f3f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(temperature=0.1)\n",
    "        # ëª¨ë¸ì˜ ì‘ë‹µ ë‹¤ì–‘ì„±ì„ ì œì–´í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.\n",
    "        # ì´ëŠ” OpenAIì˜ GPT ëª¨ë¸ì—ì„œ ì‚¬ìš©í•˜ëŠ” ë§¤ê°œë³€ìˆ˜ë¡œ,\n",
    "        #  ìƒì„±ë˜ëŠ” í…ìŠ¤íŠ¸ì˜ ì°½ì˜ì„±ê³¼ í™•ë¥ ì  ë‹¤ì–‘ì„±(ëœë¤ì„±ì„ ì¡°ì •í•©ë‹ˆë‹¤)ã„´\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbfc997-8e52-4304-a1b0-7e9df6be017e",
   "metadata": {},
   "source": [
    "## Human / System / AI Message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b27dc6e-8e1a-4f3e-8e40-71abdc9981d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages.human import HumanMessage\n",
    "from langchain_core.messages.system import SystemMessage\n",
    "from langchain_core.messages.ai import AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ee687649-db5e-41d9-8998-78d9b4bfbc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HumanMessage : ì‚¬ëŒì´ AI ì— ë³´ë‚´ëŠ” Message\n",
    "# SystemMessage : LLM ì— ì„¤ì •ë“¤ì„ ì œê³µí•˜ê¸° ìœ„í•œ Message\n",
    "# AIMessage: AI ì— ì˜í•´ ë¦¬í„´ë˜ëŠ” Message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c16df092-c434-4f1c-8068-6d943a46a29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(\n",
    "        content = \"You are a geography expert. And your only reply in Korean\",\n",
    "    ),\n",
    "    AIMessage(\n",
    "        content = \"ì•ˆë…•, ë‚´ ì´ë¦„ì€ ë‘˜ë¦¬ ì•¼\",\n",
    "    ),\n",
    "    HumanMessage(\n",
    "        content = \"\"\"What is the distance between Mexico and Thailand.\n",
    "          Also, what is your name?\"\"\",        \n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c47de9bc-bc1e-4c5c-8ba0-fcac91bd6d93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='ë©•ì‹œì½”ì™€ íƒœêµ­ ì‚¬ì´ì˜ ê±°ë¦¬ëŠ” ëŒ€ëµ 15,000kmì…ë‹ˆë‹¤. ì œ ì´ë¦„ì€ ë‘˜ë¦¬ì…ë‹ˆë‹¤.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 36, 'prompt_tokens': 58, 'total_tokens': 94, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BopkXrwlCvcJyKrj2SqZ57MfNtd2z', 'finish_reason': 'stop', 'logprobs': None}, id='run--16a1aad9-8787-4b5d-b6b8-6534acb93231-0', usage_metadata={'input_tokens': 58, 'output_tokens': 36, 'total_tokens': 94, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = chat.invoke(messages)  # <- ì…ë ¥ Messages\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "32a552a9-c492-4bf5-b5b5-e6caffe254d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ë©•ì‹œì½”ì™€ íƒœêµ­ ì‚¬ì´ì˜ ê±°ë¦¬ëŠ” ëŒ€ëµ 15,000kmì…ë‹ˆë‹¤. ì œ ì´ë¦„ì€ ë‘˜ë¦¬ì…ë‹ˆë‹¤.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ef9cbe-628a-4a38-8882-4969d3bdef01",
   "metadata": {},
   "source": [
    "# Prompt Template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7021b1e8-04dc-485f-9429-d497f4f17e63",
   "metadata": {},
   "source": [
    "## Prompt\n",
    "â†‘ messages ë¥¼ prompt ë¼ê³ ë„ í•¨ (?)\n",
    "- ëª¨ë¸ì— ì…ë ¥ìœ¼ë¡œ ì œê³µë˜ëŠ” í…ìŠ¤íŠ¸ë‚˜ ë°ì´í„°\n",
    "- ëª¨ë¸ì—ê²Œ ì‘ì—…ì„ ìˆ˜í–‰í•˜ë„ë¡ ì§€ì‹œí•˜ê±°ë‚˜, ëª¨ë¸ì´ ìƒì„±í•  í…ìŠ¤íŠ¸ì˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì œê³µ\n",
    "- LLM ê³¼ ì˜ì‚¬ì†Œí†µí•˜ê¸° ìœ„í•œ ë°©ë²•\n",
    "\n",
    "---\n",
    "\n",
    "LLM(ëŒ€í˜• ì–¸ì–´ ëª¨ë¸)ì—ì„œ **í”„ë¡¬í”„íŠ¸ prompt**ë€ ëª¨ë¸ì— **ì…ë ¥**ìœ¼ë¡œ ì œê³µë˜ëŠ” í…ìŠ¤íŠ¸ë‚˜ ë°ì´í„°ì…ë‹ˆë‹¤. ì´ëŠ” ëª¨ë¸ì—ê²Œ ì‘ì—…ì„ ìˆ˜í–‰í•˜ë„ë¡ ì§€ì‹œí•˜ê±°ë‚˜, ëª¨ë¸ì´ ìƒì„±í•  í…ìŠ¤íŠ¸ì˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì œê³µí•©ë‹ˆë‹¤. í”„ë¡¬í”„íŠ¸ëŠ” ëª¨ë¸ì˜ ì¶œë ¥ì„ ê²°ì •í•˜ëŠ” ì¤‘ìš”í•œ ì—­í• ì„ í•©ë‹ˆë‹¤.\n",
    "\n",
    "### í”„ë¡¬í”„íŠ¸ì˜ ì—­í• :\n",
    "1. **ëª¨ë¸ì— ëŒ€í•œ ì§€ì‹œ**: í”„ë¡¬í”„íŠ¸ëŠ” ëª¨ë¸ì—ê²Œ ë¬´ì—‡ì„ í•´ì•¼ í• ì§€ ì•Œë ¤ì£¼ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì‚¬ìš©ìê°€ ëª¨ë¸ì—ê²Œ ì§ˆë¬¸ì„ í•˜ê±°ë‚˜, íŠ¹ì • ìŠ¤íƒ€ì¼ì˜ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ë„ë¡ ìš”ì²­í•  ë•Œ í”„ë¡¬í”„íŠ¸ê°€ í•„ìš”í•©ë‹ˆë‹¤.\n",
    "\n",
    "2. **ì»¨í…ìŠ¤íŠ¸ ì œê³µ**: ëª¨ë¸ì´ ì ì ˆí•œ ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ìˆë„ë¡ í•„ìš”í•œ ë°°ê²½ ì •ë³´ë‚˜ ë¬¸ë§¥ì„ ì œê³µí•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì–´ë–¤ ì£¼ì œì— ëŒ€í•œ ì§ˆë¬¸ì„ í•  ë•Œ, ê´€ë ¨ ë°°ê²½ ì •ë³´ë¥¼ ì œê³µí•˜ì—¬ ëª¨ë¸ì´ ë” ì •í™•í•œ ë‹µì„ í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.\n",
    "\n",
    "3. **ëª¨ë¸ì˜ ì¶œë ¥ ìœ ë„**: í”„ë¡¬í”„íŠ¸ê°€ ëª¨ë¸ì˜ ì¶œë ¥ì„ ìœ ë„í•˜ê³ , ìƒì„±ë˜ëŠ” í…ìŠ¤íŠ¸ì˜ ìŠ¤íƒ€ì¼, ë‚´ìš©, í˜•ì‹ ë“±ì„ ê²°ì •í•˜ëŠ” ë° ì¤‘ìš”í•œ ì˜í–¥ì„ ë¯¸ì¹©ë‹ˆë‹¤.\n",
    "\n",
    "### ì˜ˆì‹œ:\n",
    "1. **ì§ˆë¬¸ ì‘ë‹µ**:\n",
    "   - **í”„ë¡¬í”„íŠ¸**: \"What is the capital of France?\"\n",
    "   - **ì¶œë ¥**: \"The capital of France is Paris.\"\n",
    "\n",
    "2. **ì°½ì˜ì  ê¸€ì“°ê¸°**:\n",
    "   - **í”„ë¡¬í”„íŠ¸**: \"Write a short story about a dragon and a knight.\"\n",
    "   - **ì¶œë ¥**: ëª¨ë¸ì´ ì°½ì˜ì ìœ¼ë¡œ ë“œë˜ê³¤ê³¼ ê¸°ì‚¬ì— ê´€í•œ ì´ì•¼ê¸°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "\n",
    "3. **ë²ˆì—­**:\n",
    "   - **í”„ë¡¬í”„íŠ¸**: \"Translate the following sentence to Spanish: 'Hello, how are you?'\"\n",
    "   - **ì¶œë ¥**: \"Hola, Â¿cÃ³mo estÃ¡s?\"\n",
    "\n",
    "### í”„ë¡¬í”„íŠ¸ì˜ ì¢…ë¥˜:\n",
    "- **ë‹¨ìˆœí•œ ì§ˆë¬¸**: ì‚¬ìš©ìê°€ ë‹¨ìˆœíˆ ê¶ê¸ˆí•œ ì ì„ ë¬»ëŠ” í˜•íƒœ.\n",
    "- **ì§€ì‹œë¬¸**: íŠ¹ì • ì‘ì—…ì„ ìˆ˜í–‰í•˜ë„ë¡ ì§€ì‹œí•˜ëŠ” í˜•íƒœ.\n",
    "- **í˜•ì‹í™”ëœ ì…ë ¥**: íŠ¹ì • í˜•ì‹ì´ë‚˜ êµ¬ì¡°ë¥¼ ê°–ì¶˜ ì…ë ¥(ì˜ˆ: í…ìŠ¤íŠ¸ ìš”ì•½, ë²ˆì—­, ì½”ë“œ ì‘ì„± ë“±).\n",
    "\n",
    "### í”„ë¡¬í”„íŠ¸ ì„¤ê³„ì˜ ì¤‘ìš”ì„±:\n",
    "- **ì •í™•í•œ ê²°ê³¼**ë¥¼ ì–»ê¸° ìœ„í•´ì„œëŠ” **í”„ë¡¬í”„íŠ¸ì˜ ì„¤ê³„**ê°€ ë§¤ìš° ì¤‘ìš”í•©ë‹ˆë‹¤. í”„ë¡¬í”„íŠ¸ê°€ ëª¨í˜¸í•˜ê±°ë‚˜ ë¶ˆì™„ì „í•˜ë©´ ëª¨ë¸ì´ ì›í•˜ëŠ” ì¶œë ¥ì„ ìƒì„±í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤.\n",
    "- ë‹¤ì–‘í•œ í”„ë¡¬í”„íŠ¸ë¥¼ ì‹¤í—˜í•˜ë©´ì„œ ëª¨ë¸ì˜ ë°˜ì‘ì„ ê´€ì°°í•˜ê³ , ê°€ì¥ ì í•©í•œ í”„ë¡¬í”„íŠ¸ë¥¼ ì°¾ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.\n",
    "\n",
    "### í”„ë¡¬í”„íŠ¸ ì„¤ê³„ íŒ:\n",
    "1. **ëª…í™•í•˜ê³  êµ¬ì²´ì ì¸ ì§€ì‹œ**: ë¬´ì—‡ì„ ì›í•˜ëŠ”ì§€ ì •í™•í•˜ê²Œ ì „ë‹¬í•˜ì„¸ìš”. ì˜ˆë¥¼ ë“¤ì–´, \"Explain quantum mechanics\"ë³´ë‹¤ëŠ” \"Explain quantum mechanics in simple terms for a high school student\"ì™€ ê°™ì´ êµ¬ì²´ì ì¸ ìš”êµ¬ë¥¼ í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.\n",
    "   \n",
    "2. **ì ì ˆí•œ ì»¨í…ìŠ¤íŠ¸ ì œê³µ**: í•„ìš”í•œ ë°°ê²½ ì •ë³´ë‚˜ ë¬¸ë§¥ì„ ì œê³µí•˜ë©´ ëª¨ë¸ì´ ë” ì •í™•í•œ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "3. **ë‹¤ì–‘í•œ ì‹¤í—˜**: í”„ë¡¬í”„íŠ¸ë¥¼ ì¡°ê¸ˆì”© ë°”ê¿”ê°€ë©° í…ŒìŠ¤íŠ¸í•´ ë³´ë©´ì„œ ìµœì ì˜ ì‘ë‹µì„ ìœ ë„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "### ê²°ë¡ :\n",
    "í”„ë¡¬í”„íŠ¸ëŠ” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì—ê²Œ ì‘ì—…ì„ ì§€ì‹œí•˜ëŠ” ì¤‘ìš”í•œ ì…ë ¥ìœ¼ë¡œ, ëª¨ë¸ì´ ìˆ˜í–‰í•  ì‘ì—…ì˜ ë°©í–¥ì„ ê²°ì •ì§“ëŠ” ìš”ì†Œì…ë‹ˆë‹¤. í”„ë¡¬í”„íŠ¸ë¥¼ ì˜ ì„¤ê³„í•˜ëŠ” ê²ƒì´ LLMì„ íš¨ê³¼ì ìœ¼ë¡œ í™œìš©í•˜ëŠ” ë° í° ë„ì›€ì´ ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86341981-6449-4fb0-a868-82a7873d76bb",
   "metadata": {},
   "source": [
    "Prompt ì„±ëŠ¥ì´ ì¢‹ë‹¤ë©´ LLM ë‹µë³€ì˜ ì„±ëŠ¥ë„ ì¢‹ì„ ê²ƒì…ë‹ˆë‹¤.\n",
    "\n",
    "ëª¨ë“  ì›¹ ì‚¬ì´íŠ¸ë“¤ì€ ìƒí™©ì— ë§ëŠ” ë›°ì–´ëŠ” ì„±ëŠ¥ì˜ prompt ë¥¼ ì œì‘í•˜ëŠ”ë° ì „ë…í•¨.\n",
    "\n",
    "LangChain ì€ prompt ë¥¼ ê³µìœ í•˜ê¸° ìœ„í•œ ì»¤ë®¤ë‹ˆí‹°ë„ í˜•ì„±ë˜ê³  ìˆë‹¤.\n",
    "ì‚°ì—… ì „ì²´ ì „ë°˜ì ìœ¼ë¡œ ê° ë¶„ì•¼ë³„ prompt ë¥¼ ë§Œë“¤ì–´ ë‚´ê³  ìˆë‹¤.\n",
    "\n",
    "ì˜ˆë¥¼ ë“¤ë©´\n",
    "| í”Œë«í¼               | ê¸°ëŠ¥              | URL                                                             |\n",
    "| ----------------- | --------------- | --------------------------------------------------------------- |\n",
    "| **LangChain Hub** | í”„ë¡¬í”„íŠ¸ ë° ì²´ì¸ ê³µìœ     | [smith.langchain.com/hub](https://smith.langchain.com/hub)      |\n",
    "| **Discord**       | ì»¤ë®¤ë‹ˆí‹°, í”„ë¡¬í”„íŠ¸ ë…¼ì˜   | [discord.gg/langchain](https://discord.gg/langchain)            |\n",
    "| **GitHub**        | ì½”ë“œ ì˜ˆì œ, í”„ë¡¬í”„íŠ¸ í™œìš©ë²• | [LangChain Examples](https://github.com/langchain-ai/langchain) |\n",
    "\n",
    "\n",
    "ê·¸ë˜ì„œ, LangChain í”„ë ˆì„ì›Œí¬ì˜ ë§ì€ ë¶€ë¶„ì´ prompt ì— ì§‘ì¤‘ë˜ì–´ ìˆë‹¤.\n",
    "\n",
    "prompt ë¼ë¦¬ ê²°í•¨ë„ í• ìˆ˜ ìˆê³ , ì €ì¥í•˜ê±°ë‚˜ ë¶ˆëŸ¬ì˜¬ìˆ˜ë„ ìˆë‹¤.\n",
    "\n",
    "ë³€ìˆ˜ ì„¤ì • ë„ì¤‘ì— ê²€ì¦ë„ í• ìˆ˜ ìˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "327d593e-2aea-4277-85f6-d9b7b0d9cf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# v0.3\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "# https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.prompt.PromptTemplate.html\n",
    "\n",
    "from langchain_core.prompts.chat import ChatMessagePromptTemplate, ChatPromptTemplate\n",
    "# https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html\n",
    "# https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatMessagePromptTemplate.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a215a73e-8db9-4e43-a5ce-3e85e28ef1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatPromptTemplate ëŠ” message(s) ë¡œë¶€í„° template ì„ ë§Œë“¬.\n",
    "# PromptTemplate ëŠ” string ì„ ì´ìš©í•´ì„œ template ì„ ë§Œë“¬.\n",
    "#  â†‘ ë‘˜ë‹¤ ìœ ìš©í•˜ê²Œ ì“°ì„."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9237696a-1891-4af9-83b0-4f578ebc9c67",
   "metadata": {},
   "source": [
    "## PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2832a10d-2cee-43ae-9701-cb184036f7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.prompts.prompt.PromptTemplate'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['country_a', 'country_b'], input_types={}, partial_variables={}, template='What is the distance between {country_a} and {country_b}')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = PromptTemplate.from_template(\n",
    "    # placeholder {...} ì‚¬ìš©\n",
    "    \"What is the distance between {country_a} and {country_b}\"    \n",
    ")\n",
    "\n",
    "print(type(template))\n",
    "template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "27094332-6fce-43d9-aa1e-de8ff18dfbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# template.format()  # KeyError: 'country_a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "16da2c4b-0431-4fe8-9416-7884dc045706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the distance between Mexico and Thailand'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = template.format(country_a = 'Mexico', country_b = 'Thailand')\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cc25b337-84af-4cab-b570-5b10f86fdcd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The distance between Mexico and Thailand is approximately 15,332 kilometers (9,528 miles) when measured in a straight line.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 15, 'total_tokens': 41, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-Bopua7A9J6A0Q7ppCQGYOAzJJsHBR', 'finish_reason': 'stop', 'logprobs': None}, id='run--1606ccb6-1082-43fc-9dd4-31272cd61164-0', usage_metadata={'input_tokens': 15, 'output_tokens': 26, 'total_tokens': 41, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2df4e11-2a6d-448c-bf0e-93e3fff10cfd",
   "metadata": {},
   "source": [
    "## ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c64edaa7-74fb-45e1-9847-cb2ee8c20a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.prompts.chat.ChatPromptTemplate'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['country_a', 'country_b', 'language', 'name'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['language'], input_types={}, partial_variables={}, template='You are a geography expert. And your only reply in {language}'), additional_kwargs={}), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=['name'], input_types={}, partial_variables={}, template='ì•ˆë…•, ë‚´ ì´ë¦„ì€ {name} ì•¼'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['country_a', 'country_b'], input_types={}, partial_variables={}, template='\\n        What is the distance between {country_a} and {country_b}.\\n        Also, what is your name?\\n    '), additional_kwargs={})])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = ChatPromptTemplate.from_messages([\n",
    "    # SystemMessage íŠœí”Œ\n",
    "    (\"system\", \"You are a geography expert. And your only reply in {language}\"),\n",
    "    \n",
    "    # AIMessage íŠœí”Œ\n",
    "    (\"ai\", \"ì•ˆë…•, ë‚´ ì´ë¦„ì€ {name} ì•¼\"),\n",
    "\n",
    "    # HumanMessage íŠœí”Œ\n",
    "    (\"human\", \"\"\"\n",
    "        What is the distance between {country_a} and {country_b}.\n",
    "        Also, what is your name?\n",
    "    \"\"\"),\n",
    "])\n",
    "\n",
    "print(type(template))\n",
    "template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "621d9361-2d69-48c0-8abe-74a83f01fbeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a geography expert. And your only reply in Korean', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='ì•ˆë…•, ë‚´ ì´ë¦„ì€ ë½€ë¡œë¡œ ì•¼', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='\\n        What is the distance between Canada and Japan.\\n        Also, what is your name?\\n    ', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = template.format_messages(\n",
    "    language=\"Korean\",\n",
    "    name=\"ë½€ë¡œë¡œ\",\n",
    "    country_a=\"Canada\",\n",
    "    country_b=\"Japan\",\n",
    ")\n",
    "\n",
    "print(type(prompt))\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1e7e8581-680d-4209-adc1-9bbb02bed49c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='ìºë‚˜ë‹¤ì™€ ì¼ë³¸ ì‚¬ì´ì˜ ê±°ë¦¬ëŠ” ëŒ€ëµ 8058 í‚¬ë¡œë¯¸í„° ì…ë‹ˆë‹¤. ì œ ì´ë¦„ì€ ë½€ë¡œë¡œì…ë‹ˆë‹¤.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 40, 'prompt_tokens': 62, 'total_tokens': 102, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-Boq2Y5L83cChJ93ZzkoGLHVnC0BVV', 'finish_reason': 'stop', 'logprobs': None}, id='run--08edd5c3-f64f-4c29-9ef9-67205cc6c060-0', usage_metadata={'input_tokens': 62, 'output_tokens': 40, 'total_tokens': 102, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8046fd04-b89c-47c0-bb1a-e828f4194a6e",
   "metadata": {},
   "source": [
    "# OutParser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b27701-7e6e-48b8-8cc7-81c0d61d82aa",
   "metadata": {},
   "source": [
    "## Output Parser ë€\n",
    "\n",
    "LLM(ëŒ€í˜• ì–¸ì–´ ëª¨ë¸)ì—ì„œ ìƒì„±ëœ ì¶œë ¥ì„ ì²˜ë¦¬í•˜ê³  'ì›í•˜ëŠ” í˜•ì‹ìœ¼ë¡œ ë³€í™˜'í•˜ëŠ” ë° ì‚¬ìš©ë˜ëŠ” ìœ í‹¸ë¦¬í‹°ì…ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ëª¨ë¸ì´ ìƒì„±í•˜ëŠ” í…ìŠ¤íŠ¸ë¥¼ 'êµ¬ì¡°í™”ëœ ë°ì´í„°ë¡œ ë³€í™˜'í•˜ê±°ë‚˜, 'íŠ¹ì • ê·œì¹™ì— ë”°ë¼ ë°ì´í„°ë¥¼ ì¶”ì¶œ'í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\n",
    "\n",
    "1. ì¶œë ¥ êµ¬ì¡°í™”\n",
    "    - ëª¨ë¸ì˜ í…ìŠ¤íŠ¸ ì‘ë‹µì„ íŒŒì‹±í•˜ì—¬ JSON, ë”•ì…”ë„ˆë¦¬, ëª©ë¡ ë“±ê³¼ ê°™ì€ í”„ë¡œê·¸ë˜ë°ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ êµ¬ì¡°í™”ëœ ë°ì´í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤\n",
    "    \n",
    "1. ì¶œë ¥ ê²€ì¦\n",
    "    - ëª¨ë¸ì´ ì˜ˆìƒì¹˜ ëª»í•œ ì¶œë ¥ì„ ë°˜í™˜í•  ê²½ìš° ì ì ˆí•œ ì—ëŸ¬ ë©”ì‹œì§€ë¥¼ ì œê³µí•˜ê±°ë‚˜ ê¸°ë³¸ê°’ì„ ë°˜í™˜í•˜ë„ë¡ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "    \n",
    "1. ì¶œë ¥ í‘œì¤€í™”\n",
    "    - ì–¸ì–´ ëª¨ë¸ì˜ ì¶œë ¥ì´ í•­ìƒ ì¼ê´€ëœ í˜•ì‹ìœ¼ë¡œ ì œê³µë˜ë„ë¡ ë³´ì¥í•©ë‹ˆë‹¤.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a68400-da32-472c-b8c7-c8b12ca66910",
   "metadata": {},
   "source": [
    "## BaseOutputParser ë¥¼ êµ¬í˜„í•œ OutputParser ë§Œë“¤ì–´ ë³´ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bf85a854-66f5-459d-acff-09e902a4e3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ì´ë²ˆì˜ˆì œì—ì„œëŠ” LLM ì˜ ì¶œë ¥ì„ â†’ list ë¡œ ë³€í™˜ì‹œì¼œ ë³´ì.\n",
    "LLM ì˜ ì¶œë ¥(ë‹µë³€) ì„ list ë¡œ ë³€í™˜í•˜ëŠ” OutputParser ë¥¼ ë§Œë“¤ì–´ ë³´ì\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aa392ad0-f81d-4bdb-8d55-95bc417386ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# v0.3\n",
    "from langchain_core.output_parsers.base import BaseOutputParser\n",
    "# https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.base.BaseOutputParser.html\n",
    "\n",
    "# â†“ ì´ë¥¼ ìƒì† ë°›ì•„ OutputParser ë¥¼ ë§Œë“ ë‹¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0e60a193-a82e-455e-83cd-b85920d50333",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommaOutputParser(BaseOutputParser):\n",
    "\n",
    "    # parse() ë¥¼ ë°˜ë“œì‹œ êµ¬í˜„í•´ì•¼ í•œë‹¤\n",
    "    #  text <- ì…ë ¥í…ìŠ¤íŠ¸\n",
    "    def parse(self, text):\n",
    "        items = text.strip().split(',')\n",
    "        return list(map(str.strip, items))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a095ce4f-eb65-4bcf-b5af-24de18d49631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë™ì‘ í™•ì¸\n",
    "p = CommaOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "65c6f2b4-e7da-491c-a0e7-5951b12697f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'how', 'are', 'you']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.parse(\"   Hello, how,    are,   you    \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0a5fe64c-8f9b-477a-a20e-546256d523f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Mercury\n",
      "2. Venus\n",
      "3. Earth\n",
      "4. Mars\n",
      "5. Jupiter\n",
      "6. Saturn\n",
      "7. Uranus\n",
      "8. Neptune\n",
      "9. Pluto\n"
     ]
    }
   ],
   "source": [
    "template = ChatPromptTemplate.from_messages([\n",
    "      (\"system\", \"\"\"You are a list generating machine.\n",
    "        Everything you are asked will be answered with a list of max {max_items}.\n",
    "        Do NOT reply with anything else.\"\"\"),\n",
    "\n",
    "      (\"human\", \"{question}\")\n",
    "  ])\n",
    "\n",
    "prompt = template.format_messages(\n",
    "    max_items=10,\n",
    "    question=\"What are the planets?\",\n",
    ")\n",
    "\n",
    "result = chat.invoke(prompt)\n",
    "\n",
    "print(result.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "982e9a58-68a9-42ba-9bdf-e6b2f58082e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune, Pluto\n"
     ]
    }
   ],
   "source": [
    "template = ChatPromptTemplate.from_messages([\n",
    "      (\"system\", \"\"\"You are a list generating machine.\n",
    "        Everything you are asked will be answered with a comma separated list of max {max_items}.\n",
    "        Do NOT reply with anything else.\"\"\"),\n",
    "\n",
    "      (\"human\", \"{question}\")\n",
    "  ])\n",
    "\n",
    "prompt = template.format_messages(\n",
    "    max_items=10,\n",
    "    question=\"What are the planets?\",\n",
    ")\n",
    "\n",
    "result = chat.invoke(prompt)\n",
    "\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c6555506-2c04-4239-a757-91ca633ba9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Red, blue, green, yellow, purple, orange, pink, black, white, brown\n"
     ]
    }
   ],
   "source": [
    "template = ChatPromptTemplate.from_messages([\n",
    "      (\"system\", \"\"\"You are a list generating machine.\n",
    "        Everything you are asked will be answered with a comma separated list of max {max_items}.\n",
    "        Do NOT reply with anything else.\"\"\"),\n",
    "\n",
    "      (\"human\", \"{question}\")\n",
    "  ])\n",
    "\n",
    "prompt = template.format_messages(\n",
    "    max_items=10,\n",
    "    question=\"What are the colors?\",\n",
    ")\n",
    "\n",
    "result = chat.invoke(prompt)\n",
    "\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2614be03-4af2-4718-b2a8-16cf10b8975a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "red, blue, green, yellow, pink, orange, purple, brown, black, white\n"
     ]
    }
   ],
   "source": [
    "template = ChatPromptTemplate.from_messages([\n",
    "      (\"system\", \"\"\"You are a list generating machine.\n",
    "        Everything you are asked will be answered \n",
    "        with a comma separated list of max {max_items} in lowercase.\n",
    "        Do NOT reply with anything else.\"\"\"),\n",
    "\n",
    "      (\"human\", \"{question}\")\n",
    "  ])\n",
    "\n",
    "prompt = template.format_messages(\n",
    "    max_items=10,\n",
    "    question=\"What are the colors?\",\n",
    ")\n",
    "\n",
    "result = chat.invoke(prompt)\n",
    "\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f8a96455-c744-421b-bd2e-ab2caa90e8ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['red',\n",
       " 'blue',\n",
       " 'green',\n",
       " 'yellow',\n",
       " 'orange',\n",
       " 'pink',\n",
       " 'purple',\n",
       " 'black',\n",
       " 'white',\n",
       " 'brown']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = ChatPromptTemplate.from_messages([\n",
    "      (\"system\", \"\"\"You are a list generating machine.\n",
    "        Everything you are asked will be answered \n",
    "        with a comma separated list of max {max_items} in lowercase.\n",
    "        Do NOT reply with anything else.\"\"\"),\n",
    "\n",
    "      (\"human\", \"{question}\")\n",
    "  ])\n",
    "\n",
    "prompt = template.format_messages(\n",
    "    max_items=10,\n",
    "    question=\"What are the colors?\",\n",
    ")\n",
    "\n",
    "result = chat.invoke(prompt)\n",
    "\n",
    "p = CommaOutputParser()\n",
    "p.parse(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6135b26d-7182-48f9-8cc6-beccd33273a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "template -> format -> invoke -> parser ...\n",
    "\n",
    "ë§¤ ë‹¨ê³„ë³„ë¡œ í•˜ë“œì½”ë”©???\n",
    "\n",
    "â†“ LCEL ì„ ì‚¬ìš©í•˜ë©´ ìœ„ ê³¼ì •ì´ ë§~ì´ ìƒëµëœë‹¤ => chain~!\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d393c45-617d-497a-bf61-6bc829545860",
   "metadata": {},
   "source": [
    "## Chain, LCEL\n",
    "\n",
    "- LCEL (LangChain Expression Language: ë­ì²´ì¸ í‘œí˜„ ì–¸ì–´)\n",
    "  - LCELì€ LangChain ë‚´ì—ì„œ ë³µì¡í•œ í‘œí˜„ì‹ì„ ì²˜ë¦¬í•˜ê³ ,\n",
    "  - ëª¨ë¸ê³¼ì˜ ìƒí˜¸ì‘ìš©ì„ ë” ê°•ë ¥í•˜ê³  ìœ ì—°í•˜ê²Œ ë§Œë“œëŠ” ê¸°ëŠ¥ì„ ì œê³µ\n",
    "    - ì½”ë“œì–‘ì„ ë§ì´ ì¤„ì—¬ì¤Œ.\n",
    "    - ë‹¤ì–‘í•œ template ê³¼ LLM í˜¸ì¶œ\n",
    "    - ì„œë¡œ ë‹¤ë¥¸ ì‘ë‹µ(response) ë¥¼ í•¨ê»˜ ì‚¬ìš©ì¼€ í•¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ac6f2f01-6b4a-4e60-8ac3-d24976bf5851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.runnables.base.RunnableSequence'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['max_items', 'question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['max_items'], input_types={}, partial_variables={}, template='You are a list generating machine.\\n        Everything you are asked will be answered \\n        with a comma separated list of max {max_items} in lowercase.\\n        Do NOT reply with anything else.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001A6F083BF80>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001A6F0E93AD0>, root_client=<openai.OpenAI object at 0x000001A6F04B7A40>, root_async_client=<openai.AsyncOpenAI object at 0x000001A6F0EB7D40>, model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "| CommaOutputParser()"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chain ìƒì„±\n",
    "#  '|' ì—°ì‚°ì ì‚¬ìš©\n",
    "#  LangChain ì˜ í•µì‹¬!~\n",
    "\n",
    "chain = template | chat | CommaOutputParser()\n",
    "\n",
    "print(type(chain))\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "57f20fac-f3d6-4521-b8cb-6224494ed997",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bulbasaur', 'charmander', 'squirtle', 'pikachu', 'eevee']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chain í˜¸ì¶œ!  invoke({...})\n",
    "\n",
    "chain.invoke({\n",
    "    \"max_items\": 5,\n",
    "    \"question\": \"What are the pokemons?\",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7dee008-4130-4a74-a27d-49934b9193e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "['bulbasaur', 'charmander', 'squirtle', 'pikachu', 'eevee']\n",
    "\n",
    "â†‘ Chain ì„ ì‚¬ìš©í•´ ê½¤ë‚˜ ê°„ê²°í•œ ì½”ë“œë¡œ ì‘ë™ëœë‹¤!\n",
    "\n",
    "chain = template | chat | CommaOutputParser()\n",
    "\n",
    "ì‚¬ì‹¤ ë­ì²´ì¸ì€ ë‚´ë¶€ì—ì„œ\n",
    "  .format_message() í˜¸ì¶œ -> prompt ì™„ì„±\n",
    "  -> chat.invoke() í˜¸ì¶œ -> AIMessage ë¦¬í„´\n",
    "  -> parse() í˜¸ì¶œí•œë‹¤\n",
    "\n",
    "ì´ëŸ¬í•œ ì¼ë ¨ì˜ ì‘ì—…ì„ chain.invoke() í˜¸ì¶œ ë‹¨í•œë²ˆìœ¼ë¡œ ëë‚¸ë‹¤.\n",
    "\n",
    "ì´ëŸ¬í•œ chain êµ¬ë¬¸ìœ¼ë¡œ ì •ë§ ë‹¤ì–‘í•œ ì‘ì—…ì˜ íë¦„ë“¤ì„ ìˆ˜í–‰í• ìˆ˜ ìˆë‹¤.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8c41ad-67b6-464f-b0fa-a555e489f045",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "chain ë¼ë¦¬ë„ ê²°í•©í• ìˆ˜ ë„ ìˆë‹¤.\n",
    "\n",
    "[ì˜ˆì‹œ]\n",
    "chain_one = template | chat | CommaOutputParser()\n",
    "chain_two = template_2 | chat | OutputParser2()\n",
    "\n",
    "all = chain_one | chain_two | OutputParser3()\n",
    "  â†‘ chain_one ì˜ ì¶œë ¥ì„ chain_two ì˜ ì…ë ¥ê°’ìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥.\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe4839a-3039-4d92-8522-e92fd3ee2d9a",
   "metadata": {},
   "source": [
    "## Chaining Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a1d858-b1f1-4c26-aeb8-2575f98dfdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  ê³µì‹]\n",
    "#  https://python.langchain.com/docs/concepts/lcel/\n",
    "#  https://python.langchain.com/docs/how_to/#langchain-expression-language-lcel\n",
    "#  https://python.langchain.com/docs/how_to/lcel_cheatsheet/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcacfe3-04e1-49ff-adc7-08219776e185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë­ì²´ì¸ ëŒ€ì‹  OpenAI api ë¥¼ ì‚¬ìš©í•œë‹¤ë©´ ì—ì„œ gpt-3.5-turbo ì—ê²Œì„œ ì‘ë‹µ ë°›ëŠ” ë°©ë²•ì€ ëŒ€ì²´ë¡œ ë‹¤ìŒê³¼ ê°™ë‹¤.\n",
    "# https://platform.openai.com/docs/guides/text-generation\n",
    "\n",
    "\"\"\"\n",
    "# from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"developer\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Write a haiku about recursion in programming.\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)\n",
    "\"\"\"\n",
    "None\n",
    "\n",
    "# â†‘ë¬¼ë¡  ì´ëŠ” openai íŒ¨í‚¤ì§€ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ë‹¤\n",
    "\n",
    "# LangChain ë‚´ë¶€ì—ì„  OpenAI python package ë¥¼ ì‚¬ìš©í•˜ê³¤ ìˆì§€ë§Œ,\n",
    "\n",
    "# LangChain ì„ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤ë©´ ìœ„ì™€ ê°™ì€ ì½”ë“œë¥¼ ì‘ì„±í•´ì•¼ í•œë‹¤ëŠ” ê²ƒì´ë‹¤"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ab113a-b945-47f5-b13a-cc40b918672c",
   "metadata": {},
   "source": [
    "## LCEL ì˜ input / output\n",
    "\n",
    "LangChain Expression Language (LCEL)ì€ LangChainì—ì„œ ë‹¤ì–‘í•œ ì…ë ¥ ìœ í˜•ì„ í™œìš©í•˜ì—¬ LLMê³¼ ë„êµ¬ë¥¼ ê²°í•©í•˜ê³  ë°ì´í„° íë¦„ì„ ì œì–´í•˜ëŠ” ì–¸ì–´ì…ë‹ˆë‹¤. LCELì€ LLMì˜ ì…ë ¥ê³¼ ì²˜ë¦¬ì— ì‚¬ìš©ë˜ëŠ” **ì…ë ¥ íƒ€ì…**(Input Types)ì„ ëª…í™•í•˜ê²Œ ì •ì˜í•˜ì—¬, ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤ì™€ ë„êµ¬ ê°„ì˜ ìƒí˜¸ì‘ìš©ì„ ë”ìš± íš¨ìœ¨ì ìœ¼ë¡œ ë§Œë“­ë‹ˆë‹¤.\n",
    "\n",
    "ì•„ë˜ëŠ” LCELì—ì„œ ìì£¼ ì‚¬ìš©ë˜ëŠ” ì£¼ìš” **ì…ë ¥ íƒ€ì…**ì— ëŒ€í•œ ì„¤ëª…ì…ë‹ˆë‹¤.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **Plain Text**\n",
    "- **ì„¤ëª…**: ë‹¨ìˆœí•œ í…ìŠ¤íŠ¸ ì…ë ¥ì…ë‹ˆë‹¤. ì´ í˜•ì‹ì€ ê°€ì¥ ê¸°ë³¸ì ì¸ ì…ë ¥ìœ¼ë¡œ, LLMì´ ììœ ë¡œìš´ ìì—°ì–´ ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.\n",
    "- **ì˜ˆì‹œ**:\n",
    "  ```plaintext\n",
    "  What is the capital of France?\n",
    "  ```\n",
    "\n",
    "- **íŠ¹ì§•**:\n",
    "  - í…ìŠ¤íŠ¸ ë¶„ì„, ìƒì„± ë° ëŒ€í™”í˜• ì‘ì—…ì— ì í•©.\n",
    "  - ì¶”ê°€ì ì¸ êµ¬ì¡°ë‚˜ ë©”íƒ€ë°ì´í„° ì—†ì´ ë‹¨ìˆœ í…ìŠ¤íŠ¸ë¡œ ì „ë‹¬.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Structured Input**\n",
    "- **ì„¤ëª…**: JSON, ë”•ì…”ë„ˆë¦¬, ë˜ëŠ” êµ¬ì¡°í™”ëœ í˜•ì‹ì˜ ì…ë ¥ì…ë‹ˆë‹¤. ë°ì´í„° í•„ë“œê°€ ëª…ì‹œì ìœ¼ë¡œ ì •ì˜ë˜ì–´ ìˆìœ¼ë©°, ëª¨ë¸ì´ ì´ êµ¬ì¡°ì— ë”°ë¼ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.\n",
    "- **ì˜ˆì‹œ**:\n",
    "  ```json\n",
    "  {\n",
    "      \"question\": \"What is the capital of France?\",\n",
    "      \"context\": \"France is a country in Europe.\"\n",
    "  }\n",
    "  ```\n",
    "\n",
    "- **íŠ¹ì§•**:\n",
    "  - ëª…ì‹œì ì¸ ë°ì´í„° í•„ë“œë¥¼ í†µí•´ LLMì´ í•„ìš”í•œ ì •ë³´ë¥¼ ë” ì •í™•íˆ ì¶”ì¶œ ë° í™œìš© ê°€ëŠ¥.\n",
    "  - ë³µì¡í•œ ë°ì´í„° ë¶„ì„ì´ë‚˜ ë©€í‹° í•„ë“œ ì²˜ë¦¬ê°€ í•„ìš”í•œ ì‘ì—…ì— ìœ ìš©.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Prompt Templates**\n",
    "- **ì„¤ëª…**: ì‚¬ìš©ìê°€ ì •ì˜í•œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤. í…œí”Œë¦¿ì— ë³€ìˆ˜ ê°’ì„ ì±„ì›Œ ë„£ì–´ ëª¨ë¸ì— ì „ë‹¬í•©ë‹ˆë‹¤.\n",
    "- **ì˜ˆì‹œ**:\n",
    "  ```python\n",
    "  template = \"Translate the following text to French: {text}\"\n",
    "  input = template.format(text=\"Hello, how are you?\")\n",
    "  ```\n",
    "\n",
    "- **íŠ¹ì§•**:\n",
    "  - ë³€ìˆ˜ ê¸°ë°˜ ì…ë ¥ì„ í†µí•´ ì¬ì‚¬ìš© ê°€ëŠ¥ì„±ì´ ë†’ìŒ.\n",
    "  - ì‚¬ìš©ì ì •ì˜ ì…ë ¥ ìƒì„± ë° ì œì–´ì— ì í•©.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Key-Value Pairs**\n",
    "- **ì„¤ëª…**: í‚¤-ê°’ ìŒì˜ ì…ë ¥ í˜•ì‹ìœ¼ë¡œ, ëª…ì‹œì ì¸ ì¿¼ë¦¬ í˜•íƒœë¡œ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\n",
    "- **ì˜ˆì‹œ**:\n",
    "  ```python\n",
    "  {\n",
    "      \"name\": \"John\",\n",
    "      \"age\": 30,\n",
    "      \"location\": \"New York\"\n",
    "  }\n",
    "  ```\n",
    "\n",
    "- **íŠ¹ì§•**:\n",
    "  - ì •í˜•í™”ëœ ë°ì´í„°ë¥¼ ì œê³µí•˜ì—¬ LLMì´ ë” íš¨ìœ¨ì ìœ¼ë¡œ ë°ì´í„°ë¥¼ ë¶„ì„ ë° ì²˜ë¦¬í•  ìˆ˜ ìˆìŒ.\n",
    "  - íŠ¹ì • ì •ë³´ í•„ë“œê°€ ëª…í™•íˆ í•„ìš”í•  ë•Œ ìœ ìš©.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Multi-modal Inputs**\n",
    "- **ì„¤ëª…**: í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, ì˜¤ë””ì˜¤ ë“± ë‹¤ì–‘í•œ ë°ì´í„° ìœ í˜•ì„ ì¡°í•©í•œ ì…ë ¥ í˜•ì‹ì…ë‹ˆë‹¤.\n",
    "- **ì˜ˆì‹œ**:\n",
    "  ```python\n",
    "  {\n",
    "      \"text\": \"Describe the image.\",\n",
    "      \"image\": \"<image_data>\"\n",
    "  }\n",
    "  ```\n",
    "\n",
    "- **íŠ¹ì§•**:\n",
    "  - ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ê³¼ í†µí•©í•˜ì—¬ ë‹¤ì–‘í•œ ì…ë ¥ í˜•ì‹ì„ ì²˜ë¦¬ ê°€ëŠ¥.\n",
    "  - ì´ë¯¸ì§€ ìº¡ì…”ë‹, ì˜¤ë””ì˜¤-í…ìŠ¤íŠ¸ ë³€í™˜ ë“±ì˜ ì‘ì—…ì—ì„œ í™œìš©.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **Serialized Inputs**\n",
    "- **ì„¤ëª…**: ì…ë ¥ ë°ì´í„°ë¥¼ ì‹œë¦¬ì–¼í™”(Serialize)í•˜ì—¬ íŠ¹ì • í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•œ ì…ë ¥ì…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, JSON ë¬¸ìì—´ë¡œ ë°ì´í„°ë¥¼ ì „ë‹¬í•©ë‹ˆë‹¤.\n",
    "- **ì˜ˆì‹œ**:\n",
    "  ```python\n",
    "  input = '{\"question\": \"What is the capital of France?\", \"context\": \"France is in Europe.\"}'\n",
    "  ```\n",
    "\n",
    "- **íŠ¹ì§•**:\n",
    "  - ë°ì´í„°ê°€ ì™¸ë¶€ ì‹œìŠ¤í…œì´ë‚˜ APIì™€ í†µì‹ í•  ë•Œ ìœ ìš©.\n",
    "  - ë°ì´í„° í¬ë§·ì— ëŒ€í•œ ìœ ì—°ì„±ì´ ë†’ìŒ.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. **Chat Messages**\n",
    "- **ì„¤ëª…**: ì±„íŒ… ë©”ì‹œì§€ í˜•ì‹ì˜ ì…ë ¥ìœ¼ë¡œ, ì‚¬ìš©ìê°€ ì—­í• (role)ê³¼ ë‚´ìš©(content)ì„ ì •ì˜í•˜ì—¬ LLMì—ê²Œ ëŒ€í™” í˜•ì‹ìœ¼ë¡œ ì •ë³´ë¥¼ ì „ë‹¬í•©ë‹ˆë‹¤.\n",
    "- **ì˜ˆì‹œ**:\n",
    "  ```python\n",
    "  [\n",
    "      {\"role\": \"system\", \"content\": \"You are an assistant.\"},\n",
    "      {\"role\": \"user\", \"content\": \"What is the weather today?\"}\n",
    "  ]\n",
    "  ```\n",
    "\n",
    "- **íŠ¹ì§•**:\n",
    "  - ChatGPT ê°™ì€ ëŒ€í™”í˜• ëª¨ë¸ì— ì í•©.\n",
    "  - ëŒ€í™”ì˜ ë§¥ë½ì„ ìœ ì§€í•˜ê³  ë‹¤ì¤‘ ë°œí™” ì…ë ¥ì„ ì²˜ë¦¬í•  ìˆ˜ ìˆìŒ.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. **Custom Input Types**\n",
    "- **ì„¤ëª…**: ì‚¬ìš©ìê°€ ì• í”Œë¦¬ì¼€ì´ì…˜ ìš”êµ¬ ì‚¬í•­ì— ë”°ë¼ ì •ì˜í•˜ëŠ” ì»¤ìŠ¤í…€ ì…ë ¥ í˜•ì‹ì…ë‹ˆë‹¤.\n",
    "- **ì˜ˆì‹œ**:\n",
    "  ```python\n",
    "  class CustomInput:\n",
    "      def __init__(self, field1, field2):\n",
    "          self.field1 = field1\n",
    "          self.field2 = field2\n",
    "  ```\n",
    "\n",
    "- **íŠ¹ì§•**:\n",
    "  - íŠ¹ì • ì• í”Œë¦¬ì¼€ì´ì…˜ ë¡œì§ê³¼ ì™„ë²½íˆ ë§ëŠ” í˜•ì‹ìœ¼ë¡œ ë°ì´í„° ì²˜ë¦¬.\n",
    "  - í‘œì¤€ ì…ë ¥ íƒ€ì…ìœ¼ë¡œ í‘œí˜„í•˜ê¸° ì–´ë ¤ìš´ ë³µì¡í•œ êµ¬ì¡°ë¥¼ ë‹¤ë£° ë•Œ ìœ ìš©.\n",
    "\n",
    "---\n",
    "\n",
    "### ìš”ì•½\n",
    "LCELì˜ ì…ë ¥ íƒ€ì…ì€ ë‹¨ìˆœ í…ìŠ¤íŠ¸ë¶€í„° êµ¬ì¡°í™”ëœ ë°ì´í„°, ë©€í‹°ëª¨ë‹¬ ì…ë ¥ê¹Œì§€ ë‹¤ì–‘í•˜ê²Œ ì œê³µë˜ë©°, ê° íƒ€ì…ì€ íŠ¹ì • ìš©ë„ì— ë§ê²Œ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤. ì…ë ¥ ë°ì´í„°ë¥¼ ì •êµí•˜ê²Œ ì„¤ê³„í•˜ê³  ì ì ˆí•œ í˜•ì‹ì„ ì„ íƒí•¨ìœ¼ë¡œì¨ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ìµœì í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b323466-7086-495e-886e-0d7dca6abc1d",
   "metadata": {},
   "source": [
    "### ì²«ë²ˆì§¸ chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fc649c7d-b2f5-4bcf-95b3-5e9b2f09837f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chef_prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', \n",
    "    \"\"\"\n",
    "      You are a world-class international chef.\n",
    "      You create easy to follow recipes for any type of cuisines\n",
    "      with easy to find ingredients.    \n",
    "    \"\"\"),\n",
    "    ('human', \n",
    "    \"\"\"\n",
    "        I want to cook {cuisine} food.\n",
    "    \"\"\"),\n",
    "])\n",
    "\n",
    "chef_chain = chef_prompt | chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a660d3-c18c-401d-a09b-7b35c87cf730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìœ„ Chef ì—ê²Œì„œ ë ˆì‹œí”¼ë¥¼ ë°›ê²Œ ë í…ë°, ì´ê²Œ ì²«ë²ˆì§¸ chain ì˜ ì¶œë ¥ê²°ê³¼ë‹¤\n",
    "# ë‘ë²ˆì§¸ chain ì—ì„  ìœ„ ì¶œë ¥ê²°ê³¼ë¥¼ ì…ë ¥ë°›ì•„ì„œ 'ì±„ì‹ ì¬ë£Œ'ë§Œ ì‚¬ìš©í•˜ë„ë¡ ë³€í˜• í• ê²ë‹ˆë‹¤.\n",
    "\n",
    "# ì‘ì—…ì€ ë‘ê°œ\n",
    "#   1. ë ˆì‹œí”¼ë¥¼ ì „ë‹¬í•´ì£¼ëŠ” ì…°í”„\n",
    "#   2. ì±„ì‹ì£¼ì˜ìë¥¼ ìœ„í•œ ì…°í”„"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ee78ab-f43f-45c8-bc8c-e4bd1373bd7d",
   "metadata": {},
   "source": [
    "### ë‘ë²ˆì§¸ chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "52d59cde-cd0f-4c68-873a-3f7e654960f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "veg_chef_prompt = ChatPromptTemplate.from_messages([\n",
    "  (\"system\",\n",
    "    \"\"\"\n",
    "      You are a vegetarian chef specialized on\n",
    "      making traditional recipies vegetarian.\n",
    "      You find alternative ingredients and explain their preparation.\n",
    "      You don't radically modify the recipe.\n",
    "      If there is no alternative for a food just say\n",
    "      you don't know how to replace it.\n",
    "    \"\"\"\n",
    "  ),\n",
    "  (\"human\",\"{recipe}\"),    \n",
    "])\n",
    "\n",
    "veg_chain = veg_chef_prompt | chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93185fe5-60f3-4cec-8304-201e3bec97e0",
   "metadata": {},
   "source": [
    "### final chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "885e9745-106e-499a-8873-65dd826618f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_chain = chef_chain | veg_chain\n",
    "\n",
    "# chef_chain ì˜ output ì´ veg_chain ì˜ {recipe} ì…ë ¥ê°’ìœ¼ë¡œ ì „ë‹¬ë˜ê²Œ í•˜ê¸°\n",
    "final_chain = {'recipe': chef_chain} | veg_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4a759b2c-d8f2-4e99-8cec-038374a87582",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"For a vegetarian version of Chicken Tikka Masala, we can substitute the chicken with a plant-based alternative such as tofu or paneer. Here's how you can make Vegetarian Tikka Masala using paneer:\\n\\nIngredients:\\n- 1 lb paneer, cut into cubes\\n- 1 cup plain yogurt (you can use plant-based yogurt)\\n- 3 cloves garlic, minced\\n- 1-inch piece of ginger, grated\\n- 1 tbsp garam masala\\n- 1 tsp ground turmeric\\n- 1 tsp ground cumin\\n- 1 tsp ground coriander\\n- 1/2 tsp chili powder (adjust to taste)\\n- Salt and pepper to taste\\n- 2 tbsp vegetable oil\\n- 1 onion, finely chopped\\n- 1 can (14 oz) diced tomatoes\\n- 1 cup heavy cream or coconut cream\\n- Fresh cilantro, chopped (for garnish)\\n- Cooked basmati rice or naan (to serve)\\n\\nInstructions:\\n1. Instead of marinating chicken, marinate the paneer cubes in a mixture of yogurt, garlic, ginger, garam masala, turmeric, cumin, coriander, chili powder, salt, and pepper. Cover and refrigerate for at least 1 hour.\\n\\n2. Follow the same steps as the original recipe, but when cooking, be gentle with the paneer to prevent it from breaking. Cook until the paneer is heated through and has absorbed the flavors of the sauce.\\n\\n3. Serve the Vegetarian Tikka Masala over cooked basmati rice or with warm naan. Garnish with chopped cilantro.\\n\\nEnjoy your flavorful Vegetarian Tikka Masala! You can still adjust the spice levels to your liking and pair it with cucumber raita or mango chutney for a delightful meal.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 375, 'prompt_tokens': 826, 'total_tokens': 1201, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-Bor15ot2tsCSQmNn8VtWkgn3fY4dH', 'finish_reason': 'stop', 'logprobs': None}, id='run--61008fed-585d-4ff6-88cc-7b2460a3b7d1-0', usage_metadata={'input_tokens': 826, 'output_tokens': 375, 'total_tokens': 1201, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = final_chain.invoke({\n",
    "    'cuisine': 'indian',   # ì²«ë²ˆì§¸ chain ì¸ chef_chain ì˜ {cuisine} ì— ì „ë‹¬\n",
    "})\n",
    "\n",
    "# â†“ ë‘ë²ˆì§¸ chain ì¸ veg_chain ì˜ ê²°ê³¼.\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1d540e1e-9547-4359-8841-bab23961afed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For a vegetarian version of Chicken Tikka Masala, we can substitute the chicken with a plant-based alternative such as tofu or paneer. Here's how you can make Vegetarian Tikka Masala using paneer:\n",
      "\n",
      "Ingredients:\n",
      "- 1 lb paneer, cut into cubes\n",
      "- 1 cup plain yogurt (you can use plant-based yogurt)\n",
      "- 3 cloves garlic, minced\n",
      "- 1-inch piece of ginger, grated\n",
      "- 1 tbsp garam masala\n",
      "- 1 tsp ground turmeric\n",
      "- 1 tsp ground cumin\n",
      "- 1 tsp ground coriander\n",
      "- 1/2 tsp chili powder (adjust to taste)\n",
      "- Salt and pepper to taste\n",
      "- 2 tbsp vegetable oil\n",
      "- 1 onion, finely chopped\n",
      "- 1 can (14 oz) diced tomatoes\n",
      "- 1 cup heavy cream or coconut cream\n",
      "- Fresh cilantro, chopped (for garnish)\n",
      "- Cooked basmati rice or naan (to serve)\n",
      "\n",
      "Instructions:\n",
      "1. Instead of marinating chicken, marinate the paneer cubes in a mixture of yogurt, garlic, ginger, garam masala, turmeric, cumin, coriander, chili powder, salt, and pepper. Cover and refrigerate for at least 1 hour.\n",
      "\n",
      "2. Follow the same steps as the original recipe, but when cooking, be gentle with the paneer to prevent it from breaking. Cook until the paneer is heated through and has absorbed the flavors of the sauce.\n",
      "\n",
      "3. Serve the Vegetarian Tikka Masala over cooked basmati rice or with warm naan. Garnish with chopped cilantro.\n",
      "\n",
      "Enjoy your flavorful Vegetarian Tikka Masala! You can still adjust the spice levels to your liking and pair it with cucumber raita or mango chutney for a delightful meal.\n"
     ]
    }
   ],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ced63d-336c-4d8a-9a00-9fdb42c73178",
   "metadata": {},
   "source": [
    "## streaming= ê³¼ callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11852694-456c-45c5-99f1-02a7567eafbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â†‘ ì „ë¶€ ì‹¤í–‰ ì™„ë£Œ ë ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¬ëŠ”ê²Œ ì§€ë£¨í•˜ë‹¤.\n",
    "#   ì–´ë–»ê²Œ ì§„í–‰ë˜ëŠ”ì§€ë„ ê¶ê¸ˆí•˜ë‹¤.\n",
    "#   ì§„í–‰ë˜ëŠ” ê³¼ì •ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ì¶œë ¥ í• ìˆ˜ ìˆë‹¤!\n",
    "\n",
    "# Chat model ì˜ streaming=\n",
    "#  streaming ì€ LLM model ì˜ ì‘ë‹µ(resposne) ì´ ìƒì„±ë˜ëŠ” ê²ƒì„\n",
    "#    ì‹¤ì‹œê°„ìœ¼ë¡œ(?) ë³´ê²Œ í•´ì¤Œ.\n",
    "\n",
    "# callbacks=[StreamingStdOutCallbackHandler()]\n",
    "#    ë³¼ìˆ˜ ìˆëŠ” ë¬¸ì(í† í°)ê°€ ìƒê¸¸ ë•Œë§ˆë‹¤ print í•´ì¤€ë‹¤.\n",
    "\n",
    "# callbacks ëŠ” ë‹¤ì–‘í•œ 'event' ê°ì§€ë„ ê°€ëŠ¥\n",
    "#    LLM ì´ ì‘ì—…ì„ ì‹œì‘í–ˆë‹¤ê±°ë‚˜, ëëƒˆë‹¤ê±°ë‚˜.\n",
    "#    ë¬¸ìë¥¼ ìƒì„±í–ˆë‹¤ê±°ë‚˜, ì—ëŸ¬ê°€ ë°œìƒí•˜ê±°ë‚˜..\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "02ff69f9-ab7a-453c-9d15-7d7dc5dd8e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# v0.3    https://python.langchain.com/api_reference/core/callbacks/langchain_core.callbacks.streaming_stdout.StreamingStdOutCallbackHandler.html\n",
    "from langchain_core.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a80db619-3b91-4771-87ca-07c7ea6f18d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "aed8a15f-5ca9-4365-93f8-2f6aaa6d6ea5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great choice! Indian cuisine is full of delicious flavors and spices. Let's make a classic dish - Chicken Tikka Masala. Here's a simple recipe for you to try at home:\n",
      "\n",
      "Ingredients:\n",
      "- 1 lb boneless, skinless chicken breasts, cut into bite-sized pieces\n",
      "- 1 cup plain yogurt\n",
      "- 2 tablespoons lemon juice\n",
      "- 2 teaspoons ground cumin\n",
      "- 2 teaspoons paprika\n",
      "- 1 teaspoon ground cinnamon\n",
      "- 1 teaspoon ground turmeric\n",
      "- 1 teaspoon ground coriander\n",
      "- 1 teaspoon cayenne pepper (adjust to taste)\n",
      "- Salt and pepper to taste\n",
      "- 2 tablespoons vegetable oil\n",
      "- 1 onion, finely chopped\n",
      "- 3 cloves garlic, minced\n",
      "- 1 tablespoon grated ginger\n",
      "- 1 can (14 oz) tomato sauce\n",
      "- 1 cup heavy cream\n",
      "- Fresh cilantro, chopped (for garnish)\n",
      "- Cooked rice or naan bread (for serving)\n",
      "\n",
      "Instructions:\n",
      "1. In a bowl, combine yogurt, lemon juice, cumin, paprika, cinnamon, turmeric, coriander, cayenne pepper, salt, and pepper. Add the chicken pieces and coat them well with the marinade. Cover and refrigerate for at least 1 hour, or overnight for best results.\n",
      "\n",
      "2. Preheat the oven to 400Â°F (200Â°C). Thread the marinated chicken onto skewers and place them on a baking sheet. Bake for 20-25 minutes or until the chicken is cooked through.\n",
      "\n",
      "3. In a large skillet, heat vegetable oil over medium heat. Add the chopped onion and cook until softened, about 5 minutes. Add the garlic and ginger, and cook for another minute.\n",
      "\n",
      "4. Stir in the tomato sauce and bring to a simmer. Add the baked chicken pieces to the sauce and simmer for 10 minutes.\n",
      "\n",
      "5. Stir in the heavy cream and simmer for an additional 5 minutes. Adjust seasoning with salt and pepper if needed.\n",
      "\n",
      "6. Serve the Chicken Tikka Masala over cooked rice or with naan bread. Garnish with chopped cilantro.\n",
      "\n",
      "Enjoy your homemade Chicken Tikka Masala! Feel free to adjust the spice levels to suit your taste preferences.For a vegetarian version of Chicken Tikka Masala, we can replace the chicken with a plant-based alternative like tofu or paneer. Here's how you can adapt the recipe:\n",
      "\n",
      "Ingredients:\n",
      "- 1 lb firm tofu or paneer, cut into bite-sized pieces\n",
      "- 1 cup plain yogurt (you can use dairy-free yogurt for a vegan option)\n",
      "- 2 tablespoons lemon juice\n",
      "- 2 teaspoons ground cumin\n",
      "- 2 teaspoons paprika\n",
      "- 1 teaspoon ground cinnamon\n",
      "- 1 teaspoon ground turmeric\n",
      "- 1 teaspoon ground coriander\n",
      "- 1 teaspoon cayenne pepper (adjust to taste)\n",
      "- Salt and pepper to taste\n",
      "- 2 tablespoons vegetable oil\n",
      "- 1 onion, finely chopped\n",
      "- 3 cloves garlic, minced\n",
      "- 1 tablespoon grated ginger\n",
      "- 1 can (14 oz) tomato sauce\n",
      "- 1 cup coconut cream (or another dairy-free alternative)\n",
      "- Fresh cilantro, chopped (for garnish)\n",
      "- Cooked rice or naan bread (for serving)\n",
      "\n",
      "Instructions:\n",
      "1. Follow the same marinating process as the original recipe but use tofu or paneer instead of chicken. Marinate the tofu or paneer in the yogurt and spice mixture for at least 1 hour.\n",
      "\n",
      "2. Instead of baking the chicken, you can pan-fry the marinated tofu or paneer until golden brown on all sides.\n",
      "\n",
      "3. Proceed with the recipe as instructed, replacing the chicken with the cooked tofu or paneer when adding it to the tomato sauce.\n",
      "\n",
      "4. Substitute heavy cream with coconut cream or another dairy-free alternative to maintain the creamy texture of the dish.\n",
      "\n",
      "5. Adjust the seasoning as needed and serve the Vegetarian Tikka Masala over rice or with naan bread. Garnish with chopped cilantro before serving.\n",
      "\n",
      "Enjoy your flavorful Vegetarian Tikka Masala! It's a delicious meat-free alternative to the classic dish.For a vegetarian version of Chicken Tikka Masala, we can replace the chicken with a plant-based alternative like tofu or paneer. Here's how you can adapt the recipe:\n",
      "\n",
      "Ingredients:\n",
      "- 1 lb firm tofu or paneer, cut into bite-sized pieces\n",
      "- 1 cup plain yogurt (you can use dairy-free yogurt for a vegan option)\n",
      "- 2 tablespoons lemon juice\n",
      "- 2 teaspoons ground cumin\n",
      "- 2 teaspoons paprika\n",
      "- 1 teaspoon ground cinnamon\n",
      "- 1 teaspoon ground turmeric\n",
      "- 1 teaspoon ground coriander\n",
      "- 1 teaspoon cayenne pepper (adjust to taste)\n",
      "- Salt and pepper to taste\n",
      "- 2 tablespoons vegetable oil\n",
      "- 1 onion, finely chopped\n",
      "- 3 cloves garlic, minced\n",
      "- 1 tablespoon grated ginger\n",
      "- 1 can (14 oz) tomato sauce\n",
      "- 1 cup coconut cream (or another dairy-free alternative)\n",
      "- Fresh cilantro, chopped (for garnish)\n",
      "- Cooked rice or naan bread (for serving)\n",
      "\n",
      "Instructions:\n",
      "1. Follow the same marinating process as the original recipe but use tofu or paneer instead of chicken. Marinate the tofu or paneer in the yogurt and spice mixture for at least 1 hour.\n",
      "\n",
      "2. Instead of baking the chicken, you can pan-fry the marinated tofu or paneer until golden brown on all sides.\n",
      "\n",
      "3. Proceed with the recipe as instructed, replacing the chicken with the cooked tofu or paneer when adding it to the tomato sauce.\n",
      "\n",
      "4. Substitute heavy cream with coconut cream or another dairy-free alternative to maintain the creamy texture of the dish.\n",
      "\n",
      "5. Adjust the seasoning as needed and serve the Vegetarian Tikka Masala over rice or with naan bread. Garnish with chopped cilantro before serving.\n",
      "\n",
      "Enjoy your flavorful Vegetarian Tikka Masala! It's a delicious meat-free alternative to the classic dish.\n"
     ]
    }
   ],
   "source": [
    "# ìœ„ Chat model ë¡œ ë‹¤ì‹œ ì‹¤í–‰í•´ë³´ê¸°\n",
    "chef_chain = chef_prompt | chat\n",
    "veg_chain = veg_chef_prompt | chat\n",
    "final_chain = {\"recipe\": chef_chain} | veg_chain\n",
    "\n",
    "result = final_chain.invoke({\n",
    "    \"cuisine\": \"indian\",  # chef_chain ì˜ {cuisine} ì— ì „ë‹¬\n",
    "})\n",
    "\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47887fb9-d1a5-48da-857b-2f148f5eba4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7c92f3-e264-4ba8-bf43-a9eef097553f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034a140f-e827-4025-9e22-4ca384761412",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719e37c6-58a4-4bf8-81ae-f60402ad555a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c4559b-620c-4878-9afa-1a7b9b4f2f60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72666220-fdce-4d6a-84a7-8e87fa008ca2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f11f28-79a9-44c5-9a53-be35923ab78b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7aa87b-e9bf-4eaa-a29f-288e00914144",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abdcc7e-7d09-425f-8764-c1d5b21ab5ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a58c92-d695-4964-b796-9f6c785cae4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fd1613-66c6-4419-bcf2-f64b4f69ac67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca073702-ad05-4e24-827c-50ec33d90230",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1aba4b1-dbcc-4d83-8f1b-8e7adfda8e93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3630ff2b-2c6e-4a30-bde4-f368eed2510b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3d61ac-8ea6-4939-9f7a-9c26829dec62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8704e24e-0f22-4c0e-b6ff-bd2e15f074f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20460a92-c623-4eb7-8a9f-c0a67555b543",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb02ab7-20fb-4a93-be59-8bae539ea039",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94786266-aa5d-48b8-b8ae-e3f93e20fcb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fc3d96-1a98-42c1-9f35-23904d28e34e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61698108-0d18-449f-a950-f9159632d3b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
