{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8f0bcf2-432b-48db-90a8-203a074221a4",
   "metadata": {},
   "source": [
    "# Memory\n",
    "\n",
    "https://python.langchain.com/api_reference/langchain/memory.html\n",
    "\n",
    "챗봇으로 하여금 대화(상태)를 '기억'하게끔 한다\n",
    "\n",
    "Memory maintains Chain state, incorporating context from past runs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c300eee3-9375-4d5f-acad-d2667af7b0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Langchain 의 memory 계층\n",
    "#  BaseMemory --> BaseChatMemory --> <name>Memory  # Examples: ZepMemory, MotorheadMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "690ed2fa-fa89-4701-b6d0-400295c5a644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI 사에서 제공하는 '기본 API' 도 랭체인 없이 사용 가능.\n",
    "# 메모리 지원하지 않는다.  이전 대화 기억 못함.  stateless 하다!\n",
    "\n",
    "# ChatGPT 서비스 는 '메모리' 기능이 탑재되어 있다.\n",
    "# 챗봇이 이전의 대화 내용과 질문을 기억하고 답할수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3a461a-c6d9-4696-83ed-62fb6ab4ed3e",
   "metadata": {},
   "source": [
    "# 기본 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f7c678c-0f26-49ad-a111-054218780d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "from langchain_core.prompts.chat import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679784c7-9eaa-43f9-8b0c-ac5939aebeea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f02e59a-7392-49f5-8c52-35b8dc3acb04",
   "metadata": {},
   "source": [
    "# ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c89aea8-24a9-48aa-975a-df42563b4196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConversationBufferMemory\n",
    "# 대화 내용 '전체'를 저장하는 메모리\n",
    "\n",
    "# 장점: 단순하다\n",
    "\n",
    "# 단점:\n",
    "# => 매번 요청할때마다 '이전 대화 기록 전체' 를 같이 보내야 함.\n",
    "#  그래야 모델이 전에 일어났던 대화를 보고 이해 할수 있다.\n",
    "#  대화내용이 길어질수록 메모리도 계속 커지니까 성능적으로도 & 비용적으로도 비효율적이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2286e5da-e2ff-4b6a-8b24-102299346941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# v0.3\n",
    "from langchain.memory.buffer import ConversationBufferMemory\n",
    "# https://python.langchain.com/api_reference/langchain/memory/langchain.memory.buffer.ConversationBufferMemory.html#conversationbuffermemory\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ad41f81-a3e2-4df3-a5e0-602284b05beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_20160\\346152885.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: Hi!\\nAI: How are you?'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = ConversationBufferMemory()\n",
    "\n",
    "memory.save_context(\n",
    "    {\"input\": \"Hi!\"},\n",
    "    {'output': 'How are you?'}\n",
    ")\n",
    "\n",
    "memory.load_memory_variables({})  # history buffer 리턴"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0308cbc-9e0b-42ea-a291-e8b0d0ea3ef3",
   "metadata": {},
   "source": [
    "## return_messages=True\n",
    "history 에  AIMessage 와 HumanMessage 로 저장된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19e4dc05-78d0-456c-b785-a7ad16daef3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Hi!', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='How are you?', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "\n",
    "memory.save_context(\n",
    "    {\"input\": \"Hi!\"},\n",
    "    {'output': 'How are you?'}\n",
    ")\n",
    "\n",
    "memory.load_memory_variables({}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7615a05c-3775-4083-984e-fe1a64a33dac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Hi!', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='How are you?', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Hi!', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='How are you?', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.save_context(\n",
    "    {\"input\": \"Hi!\"},\n",
    "    {'output': 'How are you?'}\n",
    ")\n",
    "\n",
    "memory.load_memory_variables({}) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea35cee-0b35-4147-86cc-1ef69d03cd0d",
   "metadata": {},
   "source": [
    "# ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "876c3c6a-aea6-4c75-a26d-71fb1e4779b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# v0.3\n",
    "from langchain.memory.buffer_window import ConversationBufferWindowMemory\n",
    "# https://python.langchain.com/api_reference/langchain/memory/langchain.memory.buffer_window.ConversationBufferWindowMemory.html#conversationbufferwindowmemory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b75fb2ad-67b8-4a34-8b7b-2d778787c246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConversationBufferWindowMemory 는 대화의 '특정 부분만' 을 저장하는 메모리.\n",
    "\n",
    "# 장점:\n",
    "#   메모리를 특정 크기로 유지할 수 있다!\n",
    "#   따라서 모든 대화 내용을 저장하지 않아도 된다!\n",
    "\n",
    "# 단점:\n",
    "#   챗봇이 전체 대화가 아닌 '최근 대화' 에만 집중하게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b430cc28-0133-44dc-bce9-dca457e9f2c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_20160\\2681781434.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferWindowMemory(\n"
     ]
    }
   ],
   "source": [
    "memory = ConversationBufferWindowMemory(\n",
    "    return_messages=True,\n",
    "    k=4,   # 버퍼 윈도우 사이즈,  몇개의 메세지를 저장할지 지정.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "196947a8-a2b2-4ae6-b574-1c71a91f676f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 도우미 함수 하나 준비.  여러 메세지를 추가하기 편리하니까\n",
    "def add_message(input, output):\n",
    "  memory.save_context({\"input\": input}, {\"output\": output})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "555ce793-671e-4372-b846-462ba4b43c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(\"1\", \"1\")\n",
    "add_message(\"2\", \"2\")\n",
    "add_message(\"3\", \"3\")\n",
    "add_message(\"4\", \"4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e4feb5d-bb6a-4be3-8ae3-13a63ecb69c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='1', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='1', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='2', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='2', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='3', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='3', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='4', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='4', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1502eac-68e9-4cb7-acd3-4b5d37ea35c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='2', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='2', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='3', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='3', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='4', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='4', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='5', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='5', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_message(\"5\", \"5\")\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc8a171-bae4-4a2d-a465-238f9dc5d742",
   "metadata": {},
   "source": [
    "# ConversationSummaryMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32b3d5ba-f308-44d0-885f-b5f0dd219912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConversationSummaryMemory 는 LLM 을 사용하여 대화의 요약본 (summary) 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3ca02db-9cc5-42e0-97b4-62088a031297",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2816f1b-1732-4f9f-b2f5-0941c36b228f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# v0.3\n",
    "from langchain.memory.summary import ConversationSummaryMemory\n",
    "# https://python.langchain.com/api_reference/langchain/memory/langchain.memory.summary.ConversationSummaryMemory.html#langchain.memory.summary.ConversationSummaryMemory\n",
    "\n",
    "# Continually summarizes the conversation history.\n",
    "# The summary is updated after each conversation turn.\n",
    "# The implementations returns a summary of the conversation history\n",
    "# which can be used to provide context to the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fb7e96ca-e61b-4225-b083-fcf77b7a398f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_20160\\3817587095.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationSummaryMemory(llm = llm)\n"
     ]
    }
   ],
   "source": [
    "memory = ConversationSummaryMemory(llm = llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0d1a2ddc-fbc1-44f4-8f32-e2c452c46071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 함수 하나 만들어 두고..\n",
    "\n",
    "def get_history():\n",
    "  return memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "18403428-0d6d-4fa0-94e2-0ec39d292cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# message 추가\n",
    "add_message(\n",
    "    \"Hi I'm John, I live in South Korea\",    # input\n",
    "    \"Wow that is so cool!\"  # output : AI 답변\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "23297e98-e0d9-452a-af89-f658467aa722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 또 message 추가\n",
    "add_message(\n",
    "    \"South Korea is so pretty\",\n",
    "    \"I wish I could go!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f215f1ed-f5e7-454c-9fb6-32afc902b59c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'John from South Korea introduces himself to the AI, who responds enthusiastically, finding it cool. John mentions how pretty South Korea is, and the AI expresses a wish to visit.'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d9438dd7-4264-4492-8ae5-87c426d44f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ↑ 대화를 '요약' 한 내용으로 기억하고 있다\n",
    "# 대화의 turn 이 길어질수로 summary 가 각 메세지를 효율적으료 '요약(압축)' 해준다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b44d26-eefa-4f0e-ad03-aca4fd7f6c87",
   "metadata": {},
   "source": [
    "# ConversationSummaryBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d03419fb-c9ed-4ce5-bc73-507248c06f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# v0.3\n",
    "from langchain.memory.summary_buffer import ConversationSummaryBufferMemory\n",
    "# https://python.langchain.com/api_reference/langchain/memory/langchain.memory.summary_buffer.ConversationSummaryBufferMemory.html#langchain.memory.summary_buffer.ConversationSummaryBufferMemory\n",
    "\n",
    "# Buffer with summarizer for storing conversation memory.\n",
    "# Provides a running summary of the conversation together with\n",
    "#  the most recent messages in the conversation under the constraint\n",
    "#   that the total number of tokens in the conversation does not exceed a certain limit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c2ed3aa1-f2dc-49fc-8029-88413479896a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConversationSummaryBufferMemory 는\n",
    "#   ConversationBufferMemory 와 ConversationSummaryMemory 의 결합형\n",
    "\n",
    "# 메모리에 보내온 '메세지의 수'를 지정하여 저장한다.\n",
    "# 오래된 메세지들 또한 '요약' 하여 저장함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0bcbc11a-5f6e-4ef9-8e9b-6b95858e7710",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2305747e-ac6c-4ee5-ab4e-28bab70b771c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_20160\\3924822954.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationSummaryBufferMemory(\n"
     ]
    }
   ],
   "source": [
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=150,   # 최대 가용한 메세지 토큰수 (메세지 요약되기 전)\n",
    "    return_messages=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "31b86b26-87ae-4f3b-b628-0aa1fd5862fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content=\"Hi I'm John, I live in South Korea\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Wow that is so cool!', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_message(\n",
    "    \"Hi I'm John, I live in South Korea\",    # input\n",
    "    \"Wow that is so cool!\"  # output : AI 답변\n",
    "    )\n",
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9a161b9e-87c2-47fa-aff8-bfe99c850bfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content=\"Hi I'm John, I live in South Korea\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Wow that is so cool!', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='South Korea is so pretty', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='I wish I could go!!!', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_message(\n",
    "    \"South Korea is so pretty\",\n",
    "    \"I wish I could go!!!\")\n",
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "56728c41-b229-4a4e-a1f3-3659101e5e0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content=\"Hi I'm John, I live in South Korea\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Wow that is so cool!', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='South Korea is so pretty', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='I wish I could go!!!', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='How far is Korea from Argentina?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"I don't know! Super far!\", additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_message(\n",
    "    \"How far is Korea from Argentina?\",\n",
    "    \"I don't know! Super far!\"\n",
    ")\n",
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8202495b-538f-4b72-9877-8f8a0f1638ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content=\"Hi I'm John, I live in South Korea\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Wow that is so cool!', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='South Korea is so pretty', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='I wish I could go!!!', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='How far is Korea from Argentina?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"I don't know! Super far!\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='How far is Brazil from Argentina?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"I don't know! Super far!\", additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 아래 셀을 여러차레 해보자  약 (3,4번?)\n",
    "# =>실제 '요약'이 발생할때면 Model IO 가 발생하기 때문에 시간이 좀 걸리는게 느껴질거다!\n",
    "\n",
    "add_message(\n",
    "    \"How far is Brazil from Argentina?\",\n",
    "    \"I don't know! Super far!\"\n",
    ")\n",
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3f7d9823-79bc-4902-8481-1369dec46bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# limit 에 도달하면, 오래된 메세지들이 요약되고 있을 것을 확인할수 있다. (SystemMessage 확인)\n",
    "\n",
    "# ★그러나 '요약' 이라는 과정은 API 를 사용한다는 사실을 명심하세요.\n",
    "# ★'요약' 동작은 비용 지출이 발생되는 부분입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206af78a-5d46-4465-bb10-3310fd9d23ad",
   "metadata": {},
   "source": [
    "# ConversationKGMemory\n",
    "Knowledge Graph Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0b500958-98b5-4122-a320-baca11bba15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# v0.3\n",
    "from langchain_community.memory.kg import ConversationKGMemory\n",
    "# https://python.langchain.com/api_reference/community/memory/langchain_community.memory.kg.ConversationKGMemory.html\n",
    "\n",
    "# Knowledge graph conversation memory.\n",
    "# Integrates with external knowledge graph to store and retrieve information about knowledge triples in the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "be0c83d4-7b03-4da5-9679-035804b1463a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 대화중에 '엔티티'의 knowledge graph 를 형성한다 => 가장 중요한 것들만 추출한 요약본.\n",
    "# knowledge graph 는 history 를 가지고 오지 않는다.  대신 '엔티티' 를 가지고 옴\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7c0b1b79-1cb0-4a6e-8a70-de2ba2b20972",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationKGMemory(\n",
    "    llm=llm,\n",
    "    return_messages=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8eadd9bd-9fa8-49bb-90ca-c26df40e0c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(\n",
    "    \"Hi I'm John, I live in South Korea\",    # input\n",
    "    \"Wow that is so cool!\"  # output : AI 답변\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bc5d77f1-3de1-4803-8628-b0b7270901bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content='On John: John lives in South Korea.', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({'input': 'who is John'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "afe2c45b-b4c6-4810-b681-4f24646bf3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기에 메세지를 더해보자\n",
    "add_message(\"John likes kimchi\", \"Wow that is so cool!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ae23713b-2193-4046-a6ef-77e12211ccc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content='On John: John lives in South Korea. John likes kimchi.', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({'input': 'What does John like'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385ea6cf-2383-4817-89ce-39c8d931c8ca",
   "metadata": {},
   "source": [
    "# Memory on Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b3d3cb-5635-40cf-b2b7-3ddc202cf638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메모리를 'chain' 에 꽂는 방법.\n",
    "# 두가지 형태의 'chain' 에 각각 꽂는 방법.\n",
    "#    1. off-the-shelf chain  : LangChain 에서 '일반적인 목적'을 수행하는 기본 제공되는 chain\n",
    "#    2. LCEL 사용한 chain : 커스텀 chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6154cb0-eb7f-4faa-978a-a2b95b0cbe4c",
   "metadata": {},
   "source": [
    "## LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "86d4392b-7e0c-48a2-b89f-7b92f7b696f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# off-the-shelf chain 중 하나\n",
    "# v0.3\n",
    "from langchain.chains.llm import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b9e21876-befd-43c8-a9be-c9dec0db668e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_20160\\4183264304.py:8: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=80,\n",
    ")\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    memory=memory, # ★메모리를 chain 에 꽂기!\n",
    "    prompt=PromptTemplate.from_template(\"{question}\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "045b8fb4-0f12-4013-8241-80412c8a816b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'My name is John',\n",
       " 'history': '',\n",
       " 'text': 'Nice to meet you, John! How can I assist you today?'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(input={'question':\"My name is John\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d673859a-b11a-498a-85f5-049ed9208734",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "{'question': 'My name is John',\n",
    " 'history': '',    <- 최초에는 history 없다!\n",
    " 'text': 'Nice to meet you, John! How can I assist you today?'}\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "eb67d38e-631b-4b3d-9c94-a79d401267b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'I live in Seoul',\n",
       " 'history': 'Human: My name is John\\nAI: Nice to meet you, John! How can I assist you today?',\n",
       " 'text': \"That's great! Seoul is a vibrant and bustling city with a rich history and culture. There are so many things to see and do in Seoul, from exploring ancient palaces and temples to enjoying delicious Korean cuisine and shopping in trendy neighborhoods. What do you enjoy most about living in Seoul?\"}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(input={'question':\"I live in Seoul\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc677b1-a0ac-41df-8986-6c0cc6490fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "{'question': 'I live in Seoul',\n",
    "    ↓ 이전 대화에 대한 history 가 생겼다 <- memory 가 동작한다는 뜻!\n",
    " 'history': 'Human: My name is John\\nAI: Nice to meet you, John! How can I assist you today?',\n",
    " 'text': \"That's great! Seoul is a vibrant and bustling city with a rich history and culture. There are so many things to see and do in Seoul, from exploring ancient palaces and temples to enjoying delicious Korean cuisine and shopping in trendy neighborhoods. What do you enjoy most about living in Seoul?\"}\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a1a32ec0-1f53-4342-a248-8edbed10aab0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What is my name?',\n",
       " 'history': \"System: The human introduces himself as John. The AI responds by greeting John and asking how it can assist him today.\\nHuman: I live in Seoul\\nAI: That's great! Seoul is a vibrant and bustling city with a rich history and culture. There are so many things to see and do in Seoul, from exploring ancient palaces and temples to enjoying delicious Korean cuisine and shopping in trendy neighborhoods. What do you enjoy most about living in Seoul?\",\n",
       " 'text': \"I'm sorry, I do not know your name as I am an AI assistant and do not have access to personal information.\"}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(input={'question':\"What is my name?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a15802c-b205-4232-ac91-4143f6b6a727",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "{'question': 'What is my name?',\n",
    " 'history': \"System: The human introduces himself as John. The AI responds by greeting John and asking how it can assist him today.\\nHuman: I live in Seoul\\nAI: That's great! Seoul is a vibrant and bustling city with a rich history and culture. There are so many things to see and do in Seoul, from exploring ancient palaces and temples to enjoying delicious Korean cuisine and shopping in trendy neighborhoods. What do you enjoy most about living in Seoul?\",\n",
    " 'text': \"I'm sorry, I do not know your name as I am an AI assistant and do not have access to personal information.\"}\n",
    "\n",
    "↑ 어라? 모른다고?\n",
    "\n",
    "chain 을 디버깅 해보자!\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd1742a-def2-4bd6-8ad0-816748a7a78c",
   "metadata": {},
   "source": [
    "## verbose="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e2ecdc00-5422-48eb-a2d2-92e737805270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mMy name is John\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mI live in Seoul\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWhat is my name?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'What is my name?',\n",
       " 'history': \"System: John introduces himself and mentions that he lives in Seoul. The AI responds by acknowledging Seoul's vibrant and bustling nature, rich history, and culture. It asks John what he enjoys most about living in Seoul and then learns John's name. The AI greets John by name and offers assistance.\\nAI: , the capital city of South Korea. It is a vibrant and bustling metropolis with a rich history and culture. There are so many things to see and do in Seoul, from exploring ancient palaces and temples to shopping in trendy neighborhoods and enjoying delicious Korean cuisine. I love living in this dynamic city and experiencing all that it has to offer.\",\n",
       " 'text': \"I'm sorry, I do not have access to personal information such as your name.\"}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    prompt=PromptTemplate.from_template(\"{question}\"),\n",
    "    verbose=True,  # chain 을 실행했을때 chain 의 프롬프트 로그들을 확인할수 있다. (디버깅용)\n",
    ")\n",
    "\n",
    "chain.invoke(input={'question':\"My name is John\"})\n",
    "chain.invoke(input={'question':\"I live in Seoul\"})\n",
    "chain.invoke(input={'question':\"What is my name?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9e9790-e88a-43f6-a5b3-c995dcd97ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ↑ chain 의 prompt 로그 들을 확인할 수 있다.\n",
    "# 보다시피 대화의 내역(history) 가 prompt에 계속 추가되진 않는다.\n",
    "\n",
    "# 우리가 원하는 어떤 방식으로 prompt에게 대화 기록(history)을 추가해줘야 한다!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1ed2d57f-a719-43c7-bece-00fe54abc270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: John introduces himself and mentions that he lives in Seoul. The AI responds by acknowledging Seoul's vibrant and bustling nature, rich history, and culture. It asks John what he enjoys most about living in Seoul and then learns John's name. The AI greets John by name and offers assistance, expressing its love for living in the dynamic city of Seoul and experiencing all that it has to offer.\n",
      "Human: What is my name?\n",
      "AI: I'm sorry, I do not have access to personal information such as your name.\n"
     ]
    }
   ],
   "source": [
    "# memory 에는 저장이 되어 있다\n",
    "print(memory.load_memory_variables({})['history'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0336f1cb-c66b-44e9-991b-d7b08e433c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이제 Memory 클래스 에게\n",
    "# 템플릿 앗에 콘텐츠를 넣으라고 예기 해주어야 한다.\n",
    "# ↓ 이는 memory_key= 에 \"chat_history\"(예시) 라고 말해주기만 하면 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1ab8fe-a64a-482f-857c-dffdd17b9b3c",
   "metadata": {},
   "source": [
    "## memory_key="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3f3b94ca-89e1-418d-bd00-417776f62f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=80,\n",
    "    memory_key=\"chat_history\",  # 이렇게만 지정해 놓아도 prompt template 에서 삽입 가능!\n",
    ")\n",
    "\n",
    "template = \"\"\"\n",
    "    You are a helpful AI talking to a human.\n",
    "\n",
    "    {chat_history}\n",
    "    Human:{question}\n",
    "    You:\n",
    "\"\"\"\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    memory=memory, # ★메모리를 chain 에 꽂기!\n",
    "    prompt=PromptTemplate.from_template(template),\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "24257d28-cb59-494d-8f5e-2353b9aa9d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are a helpful AI talking to a human.\n",
      "\n",
      "    \n",
      "    Human:My name is John\n",
      "    You:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'My name is John',\n",
       " 'chat_history': '',\n",
       " 'text': 'Nice to meet you, John! How can I assist you today?'}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(input={'question':\"My name is John\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5305dc17-7025-4005-acea-f500466e6058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are a helpful AI talking to a human.\n",
      "\n",
      "    Human: My name is John\n",
      "AI: Nice to meet you, John! How can I assist you today?\n",
      "    Human:I live in Seoul\n",
      "    You:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'I live in Seoul',\n",
       " 'chat_history': 'Human: My name is John\\nAI: Nice to meet you, John! How can I assist you today?',\n",
       " 'text': \"That's great to hear! How can I assist you with information or tasks related to Seoul?\"}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(input={'question':\"I live in Seoul\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d1990ef8-8834-4bb8-a133-b61b65f2c9cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are a helpful AI talking to a human.\n",
      "\n",
      "    Human: My name is John\n",
      "AI: Nice to meet you, John! How can I assist you today?\n",
      "Human: I live in Seoul\n",
      "AI: That's great to hear! How can I assist you with information or tasks related to Seoul?\n",
      "    Human:What is my name?\n",
      "    You:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'What is my name?',\n",
       " 'chat_history': \"Human: My name is John\\nAI: Nice to meet you, John! How can I assist you today?\\nHuman: I live in Seoul\\nAI: That's great to hear! How can I assist you with information or tasks related to Seoul?\",\n",
       " 'text': 'Your name is John.'}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(input={'question':\"What is my name?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b1bfd9c3-f730-4dfb-a74a-5f4e55e663e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are a helpful AI talking to a human.\n",
      "\n",
      "    Human: My name is John\n",
      "AI: Nice to meet you, John! How can I assist you today?\n",
      "Human: I live in Seoul\n",
      "AI: That's great to hear! How can I assist you with information or tasks related to Seoul?\n",
      "Human: What is my name?\n",
      "AI: Your name is John.\n",
      "    Human:My name is John\n",
      "    You:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are a helpful AI talking to a human.\n",
      "\n",
      "    System: The human introduces himself as John. The AI responds by greeting John and asking how it can assist him today.\n",
      "Human: I live in Seoul\n",
      "AI: That's great to hear! How can I assist you with information or tasks related to Seoul?\n",
      "Human: What is my name?\n",
      "AI: Your name is John.\n",
      "Human: My name is John\n",
      "AI: Nice to meet you, John! How can I assist you today?\n",
      "    Human:I live in Seoul\n",
      "    You:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'I live in Seoul',\n",
       " 'chat_history': \"System: The human introduces himself as John. The AI responds by greeting John and asking how it can assist him today.\\nHuman: I live in Seoul\\nAI: That's great to hear! How can I assist you with information or tasks related to Seoul?\\nHuman: What is my name?\\nAI: Your name is John.\\nHuman: My name is John\\nAI: Nice to meet you, John! How can I assist you today?\",\n",
       " 'text': \"That's great to hear! How can I assist you with information or tasks related to Seoul?\"}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(input={'question':\"My name is John\"})\n",
    "chain.invoke(input={'question':\"I live in Seoul\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46ae7fc-eae8-4806-b833-26c8e74747b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ↑ 지금까지는 텍스트 기반의 history  였다\n",
    "#  ChatModel 에서 Message기반의 대화기록은 어떻게 적용할 것인가?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87b40c6-b2b6-4fd1-9cfb-0718f0f4fe7d",
   "metadata": {},
   "source": [
    "# Chat Based Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d0d76371-3ac4-4a29-ba06-d4565239ba5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': \"System: John introduces himself and mentions he lives in Seoul. The AI responds by greeting John and offering assistance with information or tasks related to Seoul.\\nHuman: What is my name?\\nAI: Your name is John.\\nHuman: My name is John\\nAI: Nice to meet you, John! How can I assist you today?\\nHuman: I live in Seoul\\nAI: That's great to hear! How can I assist you with information or tasks related to Seoul?\"}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b81a2a-8b47-4f14-aed4-e1707dd7f3a7",
   "metadata": {},
   "source": [
    "## return_messages=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "882c8694-1530-4906-bd19-3f92aa0fb201",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    memory_key=\"chat_history\",  \n",
    "    return_messages=True,   # history 를 '문자열' 이 아닌 \"Message\"로 바꾸어 리턴함\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084bfb0b-6781-422b-a129-b3ec2cf3e466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 과연 prompt 에는 어떻게 대화 history 들을 넘겨줄수 있을까?\n",
    "#  단순한 하나의 텍스트가 아니라... Human Message - AI Message - Human Messagbe - AI Message - .... (여러 메세지들)\n",
    "#  심지여 요약본 발생시 System message 도 있을텐데?\n",
    "\n",
    "# prompt 에 이를 위한 공간을 어케 만드나?\n",
    "#  => MessagePlaceHolder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbfb9b0-5330-4414-bff5-02cf06066fa0",
   "metadata": {},
   "source": [
    "## MessagePlaceHolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2dae9824-0ea3-4cbe-8093-fdc35a8df080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# v0.3\n",
    "from langchain_core.prompts.chat import MessagesPlaceholder\n",
    "\n",
    "# https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html#langchain_core.prompts.chat.MessagesPlaceholder\n",
    "\n",
    "# Prompt template that assumes variable is already list of messages.\n",
    "# A placeholder which can be used to pass in a list of messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "dec505f4-cc5c-4d9e-a465-bebfe565de30",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI talking to a human\"),\n",
    "\n",
    "    # ↓ 누가 보냈는지 알수 없는, (AI? Human? System?)\n",
    "    #  예측하기 어려운 메세지들. 양과 제한 없는 메세지를 가질수 있다.\n",
    "    MessagesPlaceholder(variable_name='chat_history'),\n",
    "    \n",
    "    ('human', \"{question}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "72fa580d-9f46-43b1-92fc-7d1e4fa2797b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    prompt=prompt, # 위에서 작성한 프롬프트!\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8e147a53-af27-4896-81cf-32bac06bee1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a helpful AI talking to a human\n",
      "Human: My name is John\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'My name is John',\n",
       " 'chat_history': [HumanMessage(content='My name is John', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Hello John! How can I assist you today?', additional_kwargs={}, response_metadata={})],\n",
       " 'text': 'Hello John! How can I assist you today?'}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(input={'question':\"My name is John\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "442034a3-49f2-4bb6-8543-7f32f1819e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a helpful AI talking to a human\n",
      "Human: My name is John\n",
      "AI: Hello John! How can I assist you today?\n",
      "Human: I live in Seoul\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'I live in Seoul',\n",
       " 'chat_history': [HumanMessage(content='My name is John', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Hello John! How can I assist you today?', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='I live in Seoul', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Seoul is a vibrant city with a rich history and culture. Is there anything specific you would like to know or discuss about Seoul?', additional_kwargs={}, response_metadata={})],\n",
       " 'text': 'Seoul is a vibrant city with a rich history and culture. Is there anything specific you would like to know or discuss about Seoul?'}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(input={'question':\"I live in Seoul\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ca091572-e161-47d8-a736-09ad91b7a10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a helpful AI talking to a human\n",
      "Human: My name is John\n",
      "AI: Hello John! How can I assist you today?\n",
      "Human: I live in Seoul\n",
      "AI: Seoul is a vibrant city with a rich history and culture. Is there anything specific you would like to know or discuss about Seoul?\n",
      "Human: What is my name?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'What is my name?',\n",
       " 'chat_history': [HumanMessage(content='My name is John', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Hello John! How can I assist you today?', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='I live in Seoul', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Seoul is a vibrant city with a rich history and culture. Is there anything specific you would like to know or discuss about Seoul?', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='What is my name?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Your name is John.', additional_kwargs={}, response_metadata={})],\n",
       " 'text': 'Your name is John.'}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(input={'question':\"What is my name?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cd3f4a-512f-4162-9a2a-bfef550b0a6c",
   "metadata": {},
   "source": [
    "# LCEL Based Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5dac5899-1f43-443a-aac7-b7b5d6bcfb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful AI talking to a human\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "41b19cb5-11ba-4797-b868-790c6ba24cae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello John! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 24, 'total_tokens': 34, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BrMv1Ww0BTV6a9lt5oVKfs5kMiI9i', 'finish_reason': 'stop', 'logprobs': None}, id='run--ba17bbfa-7995-47a2-a4a4-daff272acfb6-0', usage_metadata={'input_tokens': 24, 'output_tokens': 10, 'total_tokens': 34, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | llm\n",
    "\n",
    "chain.invoke({\n",
    "    'chat_history': memory.load_memory_variables({})['chat_history'],\n",
    "    'question': 'My name is John',\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f1858f-5dd8-4327-b9b5-df6d69b3c714",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "↑ 위 방법도 가능은 하다...\n",
    "단, 위 접근 방식의 문제는\n",
    "우리가 chain 을 호출할 때마다 chat_history 도 추가해줘야 한다는 거다.\n",
    "\n",
    "↓ 이보다 더 좋은 방법도 있다. 바로 'Runnables' 라는 것을 사용하는 것이다\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667b4cc3-f652-492f-a40a-30e4b6cb958a",
   "metadata": {},
   "source": [
    "## RunnablePassThrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e0520a78-609c-4db3-afc2-a01f258ebae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# v0.3\n",
    "from langchain_core.runnables.passthrough import RunnablePassthrough\n",
    "# https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.passthrough.RunnablePassthrough.html#langchain_core.runnables.passthrough.RunnablePassthrough\n",
    "\n",
    "# Runnable to passthrough inputs unchanged or with additional keys.\n",
    "# This Runnable behaves almost like the identity function,\n",
    "#  except that it can be configured to add additional keys to the output,\n",
    "#  if the input is a dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ee60a9cd-c6f9-41d2-bb87-3c3745534fd0",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "load_memory() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[81]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      3\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m memory.load_memory_variables({})[\u001b[33m'\u001b[39m\u001b[33mchat_history\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      5\u001b[39m chain = RunnablePassthrough.assign(chat_history=load_memory)| prompt | llm\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43mchain\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mquestion\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mMy name is John\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Lang2505\\LangWork\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3045\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3043\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   3044\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i == \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m3045\u001b[39m         input_ = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3046\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3047\u001b[39m         input_ = context.run(step.invoke, input_, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Lang2505\\LangWork\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\passthrough.py:511\u001b[39m, in \u001b[36mRunnableAssign.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    504\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    505\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    506\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    509\u001b[39m     **kwargs: Any,\n\u001b[32m    510\u001b[39m ) -> \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[32m--> \u001b[39m\u001b[32m511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_invoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Lang2505\\LangWork\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1940\u001b[39m, in \u001b[36mRunnable._call_with_config\u001b[39m\u001b[34m(self, func, input_, config, run_type, serialized, **kwargs)\u001b[39m\n\u001b[32m   1936\u001b[39m     child_config = patch_config(config, callbacks=run_manager.get_child())\n\u001b[32m   1937\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   1938\u001b[39m         output = cast(\n\u001b[32m   1939\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mOutput\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m-> \u001b[39m\u001b[32m1940\u001b[39m             \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1941\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1942\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1943\u001b[39m \u001b[43m                \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1944\u001b[39m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1945\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1946\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1947\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m   1948\u001b[39m         )\n\u001b[32m   1949\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1950\u001b[39m     run_manager.on_chain_error(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Lang2505\\LangWork\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\config.py:428\u001b[39m, in \u001b[36mcall_func_with_variable_args\u001b[39m\u001b[34m(func, input, config, run_manager, **kwargs)\u001b[39m\n\u001b[32m    426\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[32m    427\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m] = run_manager\n\u001b[32m--> \u001b[39m\u001b[32m428\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Lang2505\\LangWork\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\passthrough.py:497\u001b[39m, in \u001b[36mRunnableAssign._invoke\u001b[39m\u001b[34m(self, value, run_manager, config, **kwargs)\u001b[39m\n\u001b[32m    492\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mThe input to RunnablePassthrough.assign() must be a dict.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    493\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)  \u001b[38;5;66;03m# noqa: TRY004\u001b[39;00m\n\u001b[32m    495\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m    496\u001b[39m     **value,\n\u001b[32m--> \u001b[39m\u001b[32m497\u001b[39m     **\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    502\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Lang2505\\LangWork\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3774\u001b[39m, in \u001b[36mRunnableParallel.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3769\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m get_executor_for_config(config) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[32m   3770\u001b[39m         futures = [\n\u001b[32m   3771\u001b[39m             executor.submit(_invoke_step, step, \u001b[38;5;28minput\u001b[39m, config, key)\n\u001b[32m   3772\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m key, step \u001b[38;5;129;01min\u001b[39;00m steps.items()\n\u001b[32m   3773\u001b[39m         ]\n\u001b[32m-> \u001b[39m\u001b[32m3774\u001b[39m         output = {key: \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m key, future \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(steps, futures)}\n\u001b[32m   3775\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3776\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\ProgramData\\anaconda3\\Lib\\concurrent\\futures\\_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\ProgramData\\anaconda3\\Lib\\concurrent\\futures\\_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\ProgramData\\anaconda3\\Lib\\concurrent\\futures\\thread.py:58\u001b[39m, in \u001b[36m_WorkItem.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     55\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     60\u001b[39m     \u001b[38;5;28mself\u001b[39m.future.set_exception(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Lang2505\\LangWork\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3758\u001b[39m, in \u001b[36mRunnableParallel.invoke.<locals>._invoke_step\u001b[39m\u001b[34m(step, input_, config, key)\u001b[39m\n\u001b[32m   3752\u001b[39m child_config = patch_config(\n\u001b[32m   3753\u001b[39m     config,\n\u001b[32m   3754\u001b[39m     \u001b[38;5;66;03m# mark each step as a child run\u001b[39;00m\n\u001b[32m   3755\u001b[39m     callbacks=run_manager.get_child(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmap:key:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m),\n\u001b[32m   3756\u001b[39m )\n\u001b[32m   3757\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m-> \u001b[39m\u001b[32m3758\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3759\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3760\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3761\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchild_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3762\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Lang2505\\LangWork\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:4771\u001b[39m, in \u001b[36mRunnableLambda.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   4757\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Invoke this Runnable synchronously.\u001b[39;00m\n\u001b[32m   4758\u001b[39m \n\u001b[32m   4759\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   4768\u001b[39m \u001b[33;03m    TypeError: If the Runnable is a coroutine function.\u001b[39;00m\n\u001b[32m   4769\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4770\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfunc\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m4771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4772\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_invoke\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4773\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4774\u001b[39m \u001b[43m        \u001b[49m\u001b[43mensure_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4775\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4776\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4777\u001b[39m msg = \u001b[33m\"\u001b[39m\u001b[33mCannot invoke a coroutine function synchronously.Use `ainvoke` instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4778\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Lang2505\\LangWork\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1940\u001b[39m, in \u001b[36mRunnable._call_with_config\u001b[39m\u001b[34m(self, func, input_, config, run_type, serialized, **kwargs)\u001b[39m\n\u001b[32m   1936\u001b[39m     child_config = patch_config(config, callbacks=run_manager.get_child())\n\u001b[32m   1937\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   1938\u001b[39m         output = cast(\n\u001b[32m   1939\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mOutput\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m-> \u001b[39m\u001b[32m1940\u001b[39m             \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1941\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1942\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1943\u001b[39m \u001b[43m                \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1944\u001b[39m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1945\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1946\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1947\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m   1948\u001b[39m         )\n\u001b[32m   1949\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1950\u001b[39m     run_manager.on_chain_error(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Lang2505\\LangWork\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\config.py:428\u001b[39m, in \u001b[36mcall_func_with_variable_args\u001b[39m\u001b[34m(func, input, config, run_manager, **kwargs)\u001b[39m\n\u001b[32m    426\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[32m    427\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m] = run_manager\n\u001b[32m--> \u001b[39m\u001b[32m428\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Lang2505\\LangWork\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:4629\u001b[39m, in \u001b[36mRunnableLambda._invoke\u001b[39m\u001b[34m(self, input_, run_manager, config, **kwargs)\u001b[39m\n\u001b[32m   4627\u001b[39m                 output = chunk\n\u001b[32m   4628\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4629\u001b[39m     output = \u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4630\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   4631\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4632\u001b[39m \u001b[38;5;66;03m# If the output is a Runnable, invoke it\u001b[39;00m\n\u001b[32m   4633\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, Runnable):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Lang2505\\LangWork\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\config.py:428\u001b[39m, in \u001b[36mcall_func_with_variable_args\u001b[39m\u001b[34m(func, input, config, run_manager, **kwargs)\u001b[39m\n\u001b[32m    426\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[32m    427\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m] = run_manager\n\u001b[32m--> \u001b[39m\u001b[32m428\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: load_memory() takes 0 positional arguments but 1 was given"
     ]
    }
   ],
   "source": [
    "# 메모리 변수 리턴하는 함수\n",
    "def load_memory():\n",
    "    return memory.load_memory_variables({})['chat_history']\n",
    "\n",
    "chain = RunnablePassthrough.assign(chat_history=load_memory)| prompt | llm\n",
    "\n",
    "chain.invoke({\n",
    "    'question': 'My name is John',\n",
    "})\n",
    "\n",
    "# chain 호출할때 chat_history 를 매번 기입할 필요 없어졌다.!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1e3131b9-b901-41fe-a62e-fe544b0b05b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎃 load_memory() {'question': 'My name is John'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello John! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 24, 'total_tokens': 34, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BrN42FfePVWEriYT9nQseh3HW849x', 'finish_reason': 'stop', 'logprobs': None}, id='run--22ce2035-d3c5-434c-88fc-56b168259ad5-0', usage_metadata={'input_tokens': 24, 'output_tokens': 10, 'total_tokens': 34, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_memory(input):\n",
    "    print('🎃 load_memory()', input)  # 확인용\n",
    "    return memory.load_memory_variables({})['chat_history']\n",
    "\n",
    "chain = RunnablePassthrough.assign(chat_history=load_memory)| prompt | llm\n",
    "\n",
    "chain.invoke({\n",
    "    'question': 'My name is John',\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbc7ad6-3a83-4c6f-ace3-cc27f159d55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 또 한가지 문제\n",
    "# chain 호출후 결과를 다시 메모리에 추가해야 함.\n",
    "\n",
    "# 좋은 방법은 chain 호출 함수를 따로 만들어 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f81e9d34-c2d4-47f9-8b87-de81f5985e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful AI talking to a human\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "def load_memory(input):\n",
    "  print(\"🎃load_memory()\", input)\n",
    "  return memory.load_memory_variables({})[\"chat_history\"]\n",
    "\n",
    "chain = RunnablePassthrough.assign(chat_history=load_memory) | prompt | llm\n",
    "\n",
    "def invoke_chain(question):\n",
    "    result = chain.invoke({\"question\": question})\n",
    "\n",
    "    # 체인 호출 결과를 메모리에 저장\n",
    "    memory.save_context(\n",
    "        {'input': question},\n",
    "        {'output': result.content},\n",
    "    )\n",
    "\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b57b9d98-002f-4ba5-b970-5bd879aa3481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎃load_memory() {'question': 'My name is John'}\n",
      "content='Hello John! How can I assist you today?' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 24, 'total_tokens': 34, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BrN8p6FHQTZ7HJJQBaVLlv1WRXksg', 'finish_reason': 'stop', 'logprobs': None} id='run--231388a1-d877-44b2-b0e8-29815a942a2b-0' usage_metadata={'input_tokens': 24, 'output_tokens': 10, 'total_tokens': 34, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "invoke_chain('My name is John')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "833014f6-32fe-4d40-974b-7025e94c1836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎃load_memory() {'question': 'What is my name?'}\n",
      "content='Your name is John.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 47, 'total_tokens': 52, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BrN9NsHXws4QzWQzkZjtfxZ3fDJ73', 'finish_reason': 'stop', 'logprobs': None} id='run--4537796c-e0ef-4d06-b5bd-7d5a7e1e943f-0' usage_metadata={'input_tokens': 47, 'output_tokens': 5, 'total_tokens': 52, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "825a38f6-2655-4be3-b253-e2ea340cf264",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': [HumanMessage(content='My name is John', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Hello John! How can I assist you today?', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='What is my name?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Your name is John.', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec83dcdf-b3a1-40d8-9e7a-750f6adf0855",
   "metadata": {},
   "source": [
    "## memory_key= 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "287d13e3-0dd2-4705-99b6-d067ecb0664f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎃load_memory() {'question': 'My name is John'}\n",
      "content='Hello John! How can I assist you today?' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 24, 'total_tokens': 34, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BrNEqV1nbIejba4kvtePMMvtCpT69', 'finish_reason': 'stop', 'logprobs': None} id='run--61812f7b-7e43-445e-ba58-11133e23f265-0' usage_metadata={'input_tokens': 24, 'output_tokens': 10, 'total_tokens': 34, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    # memory_key=\"chat_history\",   # memory_key=\"history\" \n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful AI talking to a human\"),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),  # 변경!\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "def load_memory(input):\n",
    "  print(\"🎃load_memory()\", input)\n",
    "  return memory.load_memory_variables({})[\"history\"]  # 변경!\n",
    "\n",
    "                                # ↓ 변경\n",
    "chain = RunnablePassthrough.assign(history=load_memory) | prompt | llm\n",
    "\n",
    "def invoke_chain(question):\n",
    "    result = chain.invoke({\"question\": question})\n",
    "\n",
    "    # 체인 호출 결과를 메모리에 저장\n",
    "    memory.save_context(\n",
    "        {'input': question},\n",
    "        {'output': result.content},\n",
    "    )\n",
    "\n",
    "    print(result)\n",
    "\n",
    "invoke_chain('My name is John')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ac876381-8941-432b-a6f7-b12cc4fbfae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎃load_memory() {'question': 'What is my name?'}\n",
      "content='Your name is John.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 47, 'total_tokens': 52, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BrNF9vgaoSsNZbKViYv1DbFu0gRnf', 'finish_reason': 'stop', 'logprobs': None} id='run--d74db093-700d-4654-b74c-145cd052374a-0' usage_metadata={'input_tokens': 47, 'output_tokens': 5, 'total_tokens': 52, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2c373e-a62c-4bc2-b3ce-72e06d80e497",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47604fdb-6a57-46ae-a0e0-37f8e9538172",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1487a47-40d9-458a-b91e-4dd0010a9b1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7a4391-af0a-4101-ac06-1f147152b38b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8749dc35-204d-4fe2-b607-ba441994023d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd5cf9a-e884-41c0-9f42-1585e1d1bb88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e133226-0a93-428f-a422-981095cc23e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfed98a-3695-4a62-a44b-6b2ca5a1f3fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38aea5f-f94f-485e-a587-c1a5fd0802a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f75854-2390-445a-a70d-1bcd3795b9e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81329f6b-c7f5-42ab-b03a-a237ec7e3e0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a8e5b9-32fb-424a-a7be-4ab220ac7cda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b29089-eefc-4aaf-b424-27164efeaf13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c2748f-3e52-4771-99b4-13663adf14af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296f7c44-ba12-426e-98d5-e8871593b1da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db24bf5-3153-45c7-abe0-f5f9926a9ba0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5840d5-a37f-44bc-800f-9b8b44d3672c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552139c0-704f-4f4f-9c8c-e4f9509a0f5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e93f50-21a5-42a5-a51a-07c5e88ddefd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
